{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ECHOES D6.2 Tech Documentation","text":"<p>This documentation is a repository of living implementation knowledge for ECHOES / CH Cloud components.</p>"},{"location":"#what-you-will-find-here","title":"What you will find here","text":"<ul> <li>a catalogue of standards and protocols (migrated from D6.2, Chapter 3),</li> <li>engineering implementation guidelines (D6.2, Chapter 6),</li> <li>monitoring patterns and example workflows (D6.2, selected sections of Chapter 7),</li> <li>templates and artefacts (D6.2, Annexes),</li> <li>validation tooling instructions and sample reports (D6.2, Chapter 11 + repository artefacts).</li> </ul>"},{"location":"engineering-handbook/code-versioning-and-open-source/","title":"Code Versioning and Open-Source Nature","text":"<p>In a large, distributed project like ECHOES, version control and disciplined release management are essential to understand who changed what, when, and why, and to prevent accidental overwrites. Equally important is an open-source posture that supports transparency, reuse, and long-term sustainability.</p>"},{"location":"engineering-handbook/code-versioning-and-open-source/#version-control-best-practices-git-semver","title":"Version Control Best Practices (Git + SemVer)","text":""},{"location":"engineering-handbook/code-versioning-and-open-source/#baseline-expectations","title":"Baseline Expectations","text":"<p>All project code is maintained in Git repositories (e.g., GitHub/GitLab/Gitea) with the following practices:</p> <ul> <li>Teams should adopt a clear branching strategy and use pull/merge requests for changes to shared branches</li> <li>Releases should follow Semantic Versioning (SemVer) to make compatibility predictable across components</li> </ul>"},{"location":"engineering-handbook/code-versioning-and-open-source/#starting-with-git-minimum-workflow","title":"Starting with Git (Minimum Workflow)","text":"<ul> <li>Commit small, focused changes with meaningful messages</li> <li>Push to a shared remote repository regularly</li> <li>Use branches to avoid direct work on <code>main</code></li> <li>Keep repository history clean and auditable (no \"dump commits\" as the norm)</li> </ul>"},{"location":"engineering-handbook/code-versioning-and-open-source/#branching-models","title":"Branching Models","text":"<p>Choose one branching model and document it clearly. A simple, effective model for many teams:</p>"},{"location":"engineering-handbook/code-versioning-and-open-source/#recommended-branch-structure","title":"Recommended Branch Structure","text":"<ul> <li><code>main</code>: Stable, production-ready code</li> <li><code>develop</code> (optional): Integration branch for upcoming release</li> <li><code>feature/&lt;name&gt;</code>: New features or major changes</li> <li><code>fix/&lt;name&gt;</code>: Bug fixes / hotfixes</li> <li><code>release/&lt;x.y.z&gt;</code>: Stabilization for a specific release (docs, final QA)</li> </ul>"},{"location":"engineering-handbook/code-versioning-and-open-source/#alternative-options","title":"Alternative Options","text":"<ul> <li>Git Flow: Suitable for larger teams with heavier processes</li> <li>Trunk-based development: Suitable for fast iteration; requires strong CI discipline</li> </ul> <p>!!! tip \"Consistency Matters\" Whichever model you choose, document it in the repository (README/CONTRIBUTING) and keep it consistent across components where feasible.</p>"},{"location":"engineering-handbook/code-versioning-and-open-source/#pull-requests-and-code-reviews","title":"Pull Requests and Code Reviews","text":"<p>All changes to shared branches (e.g., <code>main</code>, <code>develop</code>) should go through pull/merge requests that:</p> <ul> <li>Link to an issue or work item</li> <li>Describe what changed and why</li> <li>Keep changes small and reviewable when possible</li> <li>Ensure at least one peer review prior to merge</li> </ul>"},{"location":"engineering-handbook/code-versioning-and-open-source/#review-focus-areas","title":"Review Focus Areas","text":"<ul> <li>Correctness and clarity of the implementation</li> <li>Security and privacy implications of the changes</li> <li>Compatibility impact on existing systems</li> <li>Adherence to project conventions (not personal style preferences)</li> </ul>"},{"location":"engineering-handbook/code-versioning-and-open-source/#semantic-versioning-semver","title":"Semantic Versioning (SemVer)","text":"<p>Use <code>MAJOR.MINOR.PATCH</code> format (e.g., <code>v2.3.1</code>):</p> Version Component When to Increment Example MAJOR Incompatible changes (breaking API/contract behavior) <code>1.0.0</code> \u2192 <code>2.0.0</code> MINOR Backward-compatible feature additions <code>1.0.0</code> \u2192 <code>1.1.0</code> PATCH Backward-compatible bug fixes <code>1.0.0</code> \u2192 <code>1.0.1</code>"},{"location":"engineering-handbook/code-versioning-and-open-source/#recommended-artifacts-per-release","title":"Recommended Artifacts Per Release","text":"<ul> <li>Changelog entries (human-readable)</li> <li>Machine-readable version tags</li> <li>Release notes (especially for breaking changes)</li> <li>Explicit API/contract version notes where relevant</li> </ul>"},{"location":"engineering-handbook/code-versioning-and-open-source/#automation-and-ci-integration","title":"Automation and CI Integration","text":"<p>Pull/merge requests should trigger automated checks where applicable:</p> <ul> <li>Linting and static code analysis</li> <li>Unit tests</li> <li>Build and package checks</li> <li>Security scanning (SCA, secret scanning) for relevant components</li> </ul>"},{"location":"engineering-handbook/code-versioning-and-open-source/#open-source-contribution-policies","title":"Open-Source Contribution Policies","text":"<p>ECHOES software outputs are developed with an open-source approach to support transparency, reuse, and sustainability. External contributions are encouraged under a process that protects quality, security, and project integrity.</p>"},{"location":"engineering-handbook/code-versioning-and-open-source/#contribution-process","title":"Contribution Process","text":"<p>Contributions should follow this workflow:</p> <ul> <li>Flow through the issue tracker + PR/MR workflow</li> <li>Trigger automated checks (tests/lint/security scans as applicable) before merge</li> <li>Require peer review for all incoming changes</li> <li>Use an accepted contribution mechanism (e.g., CLA or DCO) where required by governance</li> </ul>"},{"location":"engineering-handbook/code-versioning-and-open-source/#non-code-contributions","title":"Non-Code Contributions","text":"<p>The following non-code contributions are explicitly in scope and welcome:</p> <ul> <li>Issue reporting and triage</li> <li>Documentation improvements</li> <li>Usability feedback and examples</li> </ul>"},{"location":"engineering-handbook/code-versioning-and-open-source/#licensing-guidance","title":"Licensing Guidance","text":"<p>A single project-wide licensing model may be agreed later. Until then, each repository should:</p> <ul> <li>Clearly declare its license</li> <li>Check dependencies for license compatibility</li> <li>Include a <code>LICENSE</code> file and (where feasible) consistent headers</li> </ul>"},{"location":"engineering-handbook/code-versioning-and-open-source/#recommended-licenses","title":"Recommended Licenses","text":"Asset Type Recommended Licenses Notes Software / source code Apache-2.0 (preferred), MIT, GPL-3.0 (case-by-case) Prefer permissive licenses for broad reuse; use copyleft only when strategically required Documentation / non-code content CC0-1.0, CC BY 4.0, CC BY-SA 4.0 Choose based on attribution and share-alike needs"},{"location":"engineering-handbook/code-versioning-and-open-source/#repository-governance-and-required-files","title":"Repository Governance and Required Files","text":"<p>Repositories should include the following baseline governance controls:</p> File / Control Purpose <code>README.md</code> Description, setup instructions, usage examples, and relevant links <code>CONTRIBUTING.md</code> How to propose changes, commit/PR expectations, and contribution guidelines <code>CODE_OF_CONDUCT.md</code> Collaboration norms and reporting path for issues Mandatory peer review Quality, maintainability, and security checks CI pipeline gates Automated tests and checks before merge Release integrity (optional) Signed releases (e.g., GPG/Sigstore) for critical components <p>!!! tip \"Maintain Consistency\" Keeping these files aligned across repositories reduces friction for contributors and reviewers, and establishes clear project standards.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/","title":"Continuous Monitoring and Feedback Loops","text":"<p>Once software is deployed, continuous monitoring ensures it remains healthy, performs well, and meets user needs. This page covers observability, real-time feedback mechanisms, and ongoing maintenance strategies.</p> <p>!!! note \"Scope and Precedence\" This page provides engineering implementation guidance for observability (logs, metrics, traces, feedback loops). Normative monitoring objectives, required signal sets, and federation-level monitoring expectations are defined in D6.2 \u00a77.2 and the relevant monitoring requirements in Chapter 8. Where overlaps exist, Chapter 7/8 are authoritative.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#implementing-telemetry-and-observability","title":"Implementing Telemetry and Observability","text":"<p>Observability means understanding what is happening inside your systems based on external outputs. The three pillars of observability work together to provide complete system insight.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#the-three-pillars-of-observability","title":"The Three Pillars of Observability","text":"Pillar Purpose What It Provides Logs Detailed records of discrete events Context for debugging specific issues Metrics Numerical measurements over time Trends, patterns, and anomaly detection Traces Request flows through distributed systems End-to-end visibility of request paths"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#why-all-three-matter","title":"Why All Three Matter","text":"<ul> <li>Logs explain what happened (and often why)</li> <li>Metrics quantify how much/how often and support alerting</li> <li>Traces show where time is spent and how requests propagate</li> </ul> <p>Together, they enable faster incident diagnosis and performance optimization.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#structured-logging","title":"Structured Logging","text":"<p>Structured logging uses a consistent, machine-parsable format (commonly JSON) rather than free-form text. This improves automated processing and cross-service correlation.</p> Benefit Practical Outcome Searchability Query by fields (e.g., <code>request_id</code>, <code>user_id</code>, <code>error_code</code>) Aggregation Produce statistics across events (error rates, top endpoints) Correlation Link related events across services via correlation IDs Automation Trigger alerts on patterns and support routing/triage"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#recommended-key-practices","title":"Recommended Key Practices","text":"<ul> <li>Include standard fields: Timestamp, severity, service, environment, <code>request_id</code> (or equivalent)</li> <li>Add domain fields where relevant (e.g., <code>object_id</code>, <code>collection</code>, <code>user_id</code>), avoiding sensitive data</li> <li>Use consistent field names across services to support unified queries and dashboards</li> <li>Propagate correlation IDs across service boundaries to enable end-to-end tracing</li> </ul>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#metrics-collection","title":"Metrics Collection","text":"<p>Metrics track numerical values over time, enabling trend analysis and alerting.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#essential-metric-types","title":"Essential Metric Types","text":"Metric Type Example Use Case Counters Total API requests Track cumulative events Gauges Active user sessions Monitor current state Histograms Request duration distribution Understand performance percentiles Summaries Request size quantiles Statistical analysis"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#common-monitoring-patterns","title":"Common Monitoring Patterns","text":"<ul> <li>RED method: Rate, Errors, Duration (for services)</li> <li>USE method: Utilization, Saturation, Errors (for resources)</li> <li>Four Golden Signals: Latency, traffic, errors, saturation</li> </ul> <p>!!! tip \"Standard Formats\" Prometheus format is widely adopted and supported by visualization tools like Grafana. Standard formats enable tool interoperability and reduce lock-in.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#distributed-tracing","title":"Distributed Tracing","text":"<p>In microservices architectures, a single user request may touch multiple services. Distributed tracing tracks the entire request flow.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#what-traces-reveal","title":"What Traces Reveal","text":"<ul> <li>Which services were involved in a request</li> <li>How long each service took to respond</li> <li>Where bottlenecks occur</li> <li>Which service failed in an error scenario</li> </ul>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#key-concepts","title":"Key Concepts","text":"<ul> <li>Span: A single operation within a trace (e.g., database query, API call)</li> <li>Trace: Collection of spans representing a complete request flow</li> <li>Context propagation: Passing trace IDs across service boundaries</li> </ul>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#implementation-considerations","title":"Implementation Considerations","text":"<ul> <li>Add minimal overhead \u2014 Tracing should not significantly slow down services</li> <li>Sample strategically \u2014 Trace all errors; sample normal requests</li> <li>Standardize on trace format \u2014 OpenTelemetry is a common cross-stack standard</li> </ul>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#monitoring-stack-architecture","title":"Monitoring Stack Architecture","text":"<p>A complete monitoring setup integrates logs, metrics, and traces.</p> <p></p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#component-roles","title":"Component Roles","text":"<ul> <li>Log aggregation (Loki / Elasticsearch): Centralize and search logs</li> <li>Metrics storage (Prometheus): Time-series storage for metrics</li> <li>Trace storage (Jaeger / Tempo): Store and query distributed traces</li> <li>Visualization (Grafana): Unified dashboards for telemetry</li> <li>Alerting (Alertmanager): Route alerts based on conditions and rules</li> </ul> <p>!!! tip \"Integrated Monitoring\" Problems are easier to diagnose when you can correlate metrics spikes with specific log entries and trace the affected requests end-to-end.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#real-time-feedback-mechanisms","title":"Real-Time Feedback Mechanisms","text":"<p>Real-time feedback during deployments helps catch issues before they affect all users.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#canary-releases","title":"Canary Releases","text":"<p>Deploy new versions to a small subset of users first, monitoring closely before full rollout.</p> <pre><code>[Load Balancer]\n\u251c\u2500 95% traffic \u2192 [Version 2.0] (stable)\n\u2514\u2500 5% traffic \u2192 [Version 2.1] (canary)\n</code></pre>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#canary-deployment-monitoring","title":"Canary Deployment Monitoring","text":"Metric Acceptable Range Action if Exceeded Error rate &lt; 1% increase Immediate rollback Response time &lt; 10% increase Investigate; possible rollback Resource usage &lt; 20% increase Monitor; adjust capacity User behavior anomalies No significant change Investigate unexpected patterns"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#rollout-strategy-example","title":"Rollout Strategy (Example)","text":"<ol> <li>Deploy to 5% of traffic</li> <li>Monitor for 15\u201330 minutes</li> <li>If stable, increase to 25%</li> <li>Continue monitoring, increase to 50%</li> <li>Complete rollout to 100%</li> </ol> <p>!!! tip \"Why Canary Releases Work\" Issues affect only a small fraction of users initially, minimizing impact while providing real-world validation.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#automated-rollback-triggers","title":"Automated Rollback Triggers","text":"<p>Define conditions that trigger automatic rollback without human intervention.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#common-trigger-conditions","title":"Common Trigger Conditions","text":"<ul> <li>Error rate exceeds threshold (e.g., 5% of requests)</li> <li>Response time increases beyond acceptable limit (e.g., p95 &gt; 2 seconds)</li> <li>Critical health check failures</li> <li>Resource exhaustion (CPU &gt; 90%, memory &gt; 95%)</li> </ul>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#benefits","title":"Benefits","text":"<ul> <li>Immediate response to problems (no waiting for human detection)</li> <li>Reduced MTTR (mean time to recovery)</li> <li>Protection from prolonged degraded user experience</li> <li>24/7 operation outside business hours</li> </ul>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#implementation-considerations_1","title":"Implementation Considerations","text":"<ul> <li>Set thresholds carefully to avoid false positives</li> <li>Ensure rollback is well-tested</li> <li>Log all automated rollbacks for post-mortem analysis</li> <li>Alert humans even when automatic rollback succeeds</li> </ul>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#feature-flags","title":"Feature Flags","text":"<p>Feature flags enable/disable functionality without redeployment, decoupling deployment from feature activation.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#use-cases","title":"Use Cases","text":"Scenario Benefit Testing in production Limited exposure to real users Quick rollback Toggle flag instead of redeploying A/B testing Different features for different user segments Gradual rollouts Increase feature exposure progressively Kill switches Disable problematic features instantly"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#implementation-patterns","title":"Implementation Patterns","text":"<ul> <li>Simple boolean flags for on/off features</li> <li>Percentage-based rollouts (e.g., show to 20% of users)</li> <li>User segment targeting (e.g., beta testers)</li> <li>Time-based flags (activate at specific times)</li> </ul> <p>!!! tip \"Deployment vs. Activation\" Traditional deployments couple code shipping with feature activation. Feature flags allow shipping code safely, then enabling features gradually (or disabling instantly if issues appear).</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#handling-service-updates-and-ongoing-maintenance","title":"Handling Service Updates and Ongoing Maintenance","text":"<p>Services require continuous care beyond initial deployment, including updates, migrations, deprecation, and end-of-life management.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#update-strategies","title":"Update Strategies","text":""},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#zero-downtime-updates-for-critical-services","title":"Zero-Downtime Updates (for Critical Services)","text":"Technique How It Works Best For Rolling updates Replace instances gradually, one at a time Stateless services Blue-green deployment Two identical environments; switch traffic instantly Mission-critical services needing instant rollback Database migrations Backward-compatible changes in multiple steps Schema changes that must remain compatible"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#maintenance-windows-for-complex-updates","title":"Maintenance Windows (for Complex Updates)","text":"<p>Best practices for scheduled maintenance:</p> <ul> <li>Schedule during lowest-usage periods (analyze traffic patterns first)</li> <li>Announce well in advance (minimum 1 week for minor; 1 month for major)</li> <li>Provide status updates during maintenance</li> <li>Have a tested rollback plan ready</li> <li>Communicate completion and any residual issues</li> </ul>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#api-deprecation-process","title":"API Deprecation Process","text":"<p>When deprecating APIs or features, follow a phased approach respecting user needs.</p> Phase Timeline Actions Phase 1: Announce deprecation 3\u20136 months before removal Deprecation notice in docs; optionally headers in responses; direct notifications to known users; explain alternatives Phase 2: Provide migration guidance During deprecation Examples, migration tooling if possible, dedicated support Phase 3: Sunset period Final 1\u20132 months Deprecation warnings; track usage; assist remaining users Phase 4: Final removal After deadline Remove only after announced deadline; return clear errors pointing to alternatives; retain explanatory documentation"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#dependency-updates","title":"Dependency Updates","text":"<p>Regular dependency updates are essential for security, stability, and performance.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#update-strategy-by-severity","title":"Update Strategy by Severity","text":"Update Type Timeline Approach Security patches Apply immediately (within 24\u201348 hours) Test quickly; deploy urgently Minor updates Review and apply monthly Batch updates; test normally Major updates Plan carefully; test thoroughly Dedicated effort; extensive testing"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#automation-tools","title":"Automation Tools","text":"<p>Examples: Dependabot, Renovate, Snyk</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#best-practices","title":"Best Practices","text":"<ul> <li>Enable automated PRs but require review before merging</li> <li>Test dependency updates in staging before production</li> <li>Group related updates together</li> <li>Monitor after deployment for unexpected issues</li> <li>Keep dependencies current to avoid large, risky upgrades</li> </ul>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#monitoring-technical-debt","title":"Monitoring Technical Debt","text":"<p>Technical debt should be managed as part of Continual Service Improvement (CSI) to protect service reliability, maintainability, and delivery velocity. Debt items should be visible, owned, and periodically reviewed.</p>"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#what-to-track","title":"What to Track","text":"Category Examples Refactoring needs Complex modules, duplicated code, architectural drift Outdated dependencies EOL components, delayed upgrades, known vulnerabilities Test coverage gaps Missing critical-path tests, flaky tests Performance bottlenecks Slow queries, scaling constraints Documentation/ops gaps Missing runbooks, unclear procedures, outdated docs"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#prioritization-criteria","title":"Prioritization Criteria","text":"Criterion Guiding Question Impact What degrades or breaks if not fixed? Effort How long/complex is the fix? Risk How likely it causes incidents/security issues? Dependencies Does it block or slow other planned work?"},{"location":"engineering-handbook/continuous-monitoring-and-feedback-loops/#technical-debt-register","title":"Technical Debt Register","text":"<p>Maintain a Technical Debt Register (CSI backlog) and review it regularly in planning.</p> <p>Minimum expectations:</p> Requirement Minimum Expectation Visibility Central list in the issue tracker (tag/label: <code>tech-debt</code>) Ownership Each item has an accountable owner Review cadence Reviewed each sprint or monthly (team standard) Capacity allocation Reserve dedicated time (e.g., ~20% of sprint capacity) for debt reduction <p>!!! warning \"High-Risk Items\" High-risk items (e.g., security exposures, EOL dependencies, critical runbook gaps) should not be deferred without documented justification and a review date.</p>"},{"location":"engineering-handbook/deployment-environments-and-versioning/","title":"Deployment, Environments, and Versioning Management","text":"<p>In a multi-partner project, deployments must be predictable, repeatable, and easy to troubleshoot. They must also accommodate different infrastructure choices and operational maturity across consortium members. The core principles are consistent across component types: deploy in a controlled way, automate where feasible, and make failures easy to detect and recover from.</p> <p>!!! note \"Normative Boundary\" This page is implementation guidance (Living Documentation). It does not create new normative requirements beyond those defined in D6.2 Chapter 8 and validated via Chapters 9\u201311.</p>"},{"location":"engineering-handbook/deployment-environments-and-versioning/#deployment-pipelines-and-rollback-strategies","title":"Deployment Pipelines and Rollback Strategies","text":"<p>Every service/component should be deployed via a pipeline: a repeatable sequence that automates testing, packaging, and deployment.</p>"},{"location":"engineering-handbook/deployment-environments-and-versioning/#typical-pipeline-stages","title":"Typical Pipeline Stages","text":"<p>A deployment pipeline typically includes the following stages:</p> <ol> <li>Run unit and integration tests to validate code quality</li> <li>Package the application (e.g., container image, binary, Python wheel)</li> <li>Tag the release using Semantic Versioning</li> <li>Deploy to staging/test environment for validation</li> <li>Promote to production via manual or automated approval</li> </ol>"},{"location":"engineering-handbook/deployment-environments-and-versioning/#rollback-principles","title":"Rollback Principles","text":"<p>Rollback should be fast and safe. Key principles include:</p> <ul> <li>Deployments should be atomic and reversible to minimize risk</li> <li>Previous versions remain available (keep at least the last 2\u20133 versions)</li> <li>Rollback procedures are tested periodically to ensure they work when needed</li> <li>Post-deploy monitoring and escalation paths are defined for quick issue detection</li> </ul> <p>!!! tip \"Immutable Artifacts\" Immutable artifacts (e.g., versioned container images) reduce \"it works on my machine\" drift and simplify rollback procedures.</p>"},{"location":"engineering-handbook/deployment-environments-and-versioning/#deployment-strategies","title":"Deployment Strategies","text":"Strategy Description Typical Use Case Rolling update Gradually replace old instances with new ones Standard deployments with minimal downtime Blue-green Two identical environments; switch traffic instantly Near zero-downtime, simple rollback Canary Deploy to a small subset of traffic/users first Risk mitigation for critical services Recreate Stop old version, start new version Simpler deployments where downtime is acceptable"},{"location":"engineering-handbook/deployment-environments-and-versioning/#infrastructure-as-code-iac","title":"Infrastructure as Code (IaC)","text":"<p>Managing infrastructure manually is risky and hard to audit. Prefer Infrastructure as Code (IaC) so that environments are reproducible, reviewable, versioned, and shareable across teams.</p>"},{"location":"engineering-handbook/deployment-environments-and-versioning/#benefits-of-iac","title":"Benefits of IaC","text":"<ul> <li>Consistent environments across test, staging, and production</li> <li>Auditable change history with version control integration</li> <li>Fast rebuild in case of failure or disaster recovery</li> <li>Portable blueprints across providers (cloud/on-premises)</li> </ul>"},{"location":"engineering-handbook/deployment-environments-and-versioning/#tooling-overview","title":"Tooling Overview","text":"Tool Focus Language Best For Terraform Infrastructure provisioning HCL Cloud resources (VMs, networks, storage) Ansible Configuration management YAML Installing software, managing configurations"},{"location":"engineering-handbook/deployment-environments-and-versioning/#lightweight-deployment-options","title":"Lightweight Deployment Options","text":"<p>Even lightweight deployments benefit from codification:</p> <ul> <li>Docker Compose for small, single-host deployments</li> <li>Kubernetes manifests/Helm for orchestrated services</li> </ul>"},{"location":"engineering-handbook/deployment-environments-and-versioning/#test-automation-for-deployment-validation","title":"Test Automation for Deployment Validation","text":"<p>Before a release is promoted to production, it should pass automated deployment validation tests that confirm it works in a production-like environment with real dependencies and configuration.</p>"},{"location":"engineering-handbook/deployment-environments-and-versioning/#what-deployment-validation-confirms","title":"What Deployment Validation Confirms","text":"Validation Question Examples of Evidence Can the service start in the target environment? Successful startup; readiness probe passes Is configuration complete and correct? Required env vars/secrets present; config schema checks Can the service connect to dependencies? DB/queue/cache connectivity checks; timeouts within limits Does authn/authz work end-to-end? Token validation against real IdP; role-based access checks Does it respond correctly to real requests? Key endpoints return expected status codes and payload shapes"},{"location":"engineering-handbook/deployment-environments-and-versioning/#recommended-validation-test-types","title":"Recommended Validation Test Types","text":"Test Type Purpose Typical Examples Notes Health checks Confirm service is running and responsive Liveness/readiness endpoints; basic HTTP 200 Keep lightweight; avoid external dependencies Smoke tests Verify essential workflows quickly Minimal CRUD path; search returns results; auth validates tokens Breadth over depth; use controlled test data Dependency integration checks Confirm connectivity/compatibility DB migrations; queue publish/consume; external API reachability Catches network/config issues mocks miss Security &amp; compliance checks Ensure controls apply in deployed environment HTTPS enforced; auth required; security headers; audit logging Focused on deployment-relevant controls"},{"location":"engineering-handbook/deployment-environments-and-versioning/#design-principles-for-validation-tests","title":"Design Principles for Validation Tests","text":"Principle Guideline Speed Complete in minutes to avoid blocking delivery Reliability Deterministic; flaky tests fixed or quarantined Clear failures Errors identify failing check, endpoint, and relevant dependency/config Automation and gating Run automatically in pipeline; failures block promotion"},{"location":"engineering-handbook/deployment-environments-and-versioning/#pipeline-integration","title":"Pipeline Integration","text":"<p>Deployment validation should be integrated into your CI/CD pipeline with the following practices:</p> <ul> <li>Run automatically after deployment to test/staging environments</li> <li>Act as a promotion gate to prevent faulty releases from reaching production</li> <li>Retain results as CI artifacts/dashboards for historical tracking</li> <li>Notify on failures with links to logs, test reports, and deployed version metadata</li> </ul>"},{"location":"engineering-handbook/deployment-environments-and-versioning/#semantic-versioning-and-immutable-artifacts","title":"Semantic Versioning and Immutable Artifacts","text":"<p>All deployable components should follow Semantic Versioning (SemVer) to make compatibility expectations explicit (e.g., <code>v2.1.0</code> indicates major/minor/patch semantics).</p>"},{"location":"engineering-handbook/deployment-environments-and-versioning/#immutable-artifacts","title":"Immutable Artifacts","text":"<p>Released artifacts should be immutable:</p> <ul> <li>Once built and tagged, they must not change</li> <li>If a rebuild is required, publish a new version tag</li> <li>Never modify existing releases in place</li> </ul>"},{"location":"engineering-handbook/deployment-environments-and-versioning/#versioning-best-practices","title":"Versioning Best Practices","text":"<ul> <li>Tag releases in Git (e.g., <code>git tag v2.1.0</code>) for traceability</li> <li>Expose version information via <code>/version</code> endpoint or <code>version.txt</code> file</li> <li>Keep the same version across all artifacts (image, docs, API spec)</li> <li>Generate changelogs from commit metadata (e.g., Conventional Commits + changelog tooling)</li> <li>Never delete or overwrite published artifacts</li> </ul> <p>!!! tip \"Version Consistency\" Maintaining consistent version numbers across all artifacts (container images, documentation, API specifications) makes debugging and traceability much easier.</p>"},{"location":"engineering-handbook/development-processes-and-automation/","title":"Development Processes and Automation","text":"<p>Modern software development is not only about writing code, but about how code is developed, tested, reviewed, and delivered. In ECHOES, we aim to reduce human error, improve collaboration, and move faster without compromising quality. This is enabled by well-defined development processes and automation at every step.</p> <p>!!! note \"Normative Boundary\" This is implementation guidance (Living Documentation). The canonical interoperability requirements remain in D6.2 Chapter 8 and are validated via Chapters 9\u201311.</p>"},{"location":"engineering-handbook/development-processes-and-automation/#code-reviews","title":"Code Reviews","text":"<p>Code reviews improve software quality and shared knowledge. Every change affecting a shared branch should be submitted via a pull/merge request (PR/MR) and reviewed by at least one other team member before merge. Reviews should focus on clarity, correctness, and maintainability\u2014not only whether the code runs.</p>"},{"location":"engineering-handbook/development-processes-and-automation/#pull-request-workflow","title":"Pull Request Workflow","text":"<p>The PR workflow should follow these stages:</p> <ol> <li>Create a branch (<code>feature/&lt;name&gt;</code> or <code>fix/&lt;ticket&gt;</code>)</li> <li>Commit changes with clear, conventional messages</li> <li>Open a PR/MR linking to the relevant issue; describe what changed and why</li> <li>Automated checks run (CI tests, linting, security checks where applicable)</li> <li>Peer review with discussion, revisions, and confirmation of design/quality/standards</li> <li>Approval when criteria are met</li> <li>Merge into <code>develop</code> or <code>main</code></li> </ol> <p></p>"},{"location":"engineering-handbook/development-processes-and-automation/#review-checklist","title":"Review Checklist","text":"Review Aspect Key Questions Common Red Flags Correctness Does it solve the intended problem? Does it handle edge cases? Logic errors, missing validation, incomplete implementation Clarity Is it easy to understand? Are names meaningful? Cryptic names, complex nested logic, missing context Security Are there vulnerabilities? Are secrets properly managed? Injection risks, hardcoded credentials, exposed sensitive data Testing Are tests adequate? Do all tests pass? Missing coverage, failing tests, untested edge cases Documentation Are complex sections explained? Is the API documented? Undocumented complex logic, missing API specs, unclear usage Standards compliance Does it follow conventions? Meets interoperability guidance? Inconsistent formatting, non-standard patterns, L1/L2/L3 violations"},{"location":"engineering-handbook/development-processes-and-automation/#review-etiquette","title":"Review Etiquette","text":"Principle Do Don't Be respectful \"This function could be clearer if we\u2026\" \"This code is terrible\" Focus on code \"This variable name could be more descriptive\" \"You always write unclear code\" Explain why \"Let's extract this to a function because it's used in 3 places\u2026\" \"This should be a function\" (no reasoning) Acknowledge good work \"Great error handling here!\" Only pointing out problems Resolve conflicts constructively \"Let's hop on a quick call to discuss this approach\" 20+ comment threads arguing back and forth"},{"location":"engineering-handbook/development-processes-and-automation/#best-practices","title":"Best Practices","text":""},{"location":"engineering-handbook/development-processes-and-automation/#for-pr-authors","title":"For PR Authors","text":"<ul> <li>Keep PRs small and focused (e.g., &lt; 400 lines when possible)</li> <li>Link to a tracked issue or user story for context</li> <li>Write clear PR descriptions explaining what changed and why</li> <li>Respond to feedback promptly and professionally</li> <li>Mark conversations as resolved when addressed</li> </ul>"},{"location":"engineering-handbook/development-processes-and-automation/#for-reviewers","title":"For Reviewers","text":"<ul> <li>Review PRs within 24 hours when possible</li> <li>Start with positive observations to encourage good practices</li> <li>Ask questions rather than making demands</li> <li>Suggest alternatives with reasoning to facilitate learning</li> <li>Approve when standards are met, even if you would do it differently</li> </ul>"},{"location":"engineering-handbook/development-processes-and-automation/#unit-integration-and-end-to-end-testing","title":"Unit, Integration, and End-to-End Testing","text":"<p>Automated testing catches issues early and enables safer changes over time. ECHOES recommends three tiers of tests: unit, integration, and end-to-end (E2E). Where feasible, tests should run automatically in CI/CD with results reported in an accessible way (CI summaries, dashboards, or reporting tools).</p>"},{"location":"engineering-handbook/development-processes-and-automation/#test-tiers-overview","title":"Test Tiers Overview","text":"Tier Purpose Typical Scope Environment Typical Tools Unit Validate small, isolated logic Functions/classes/modules Local + CI pytest, JUnit, Jest Integration Verify component interactions APIs, DB, queues, file I/O CI + ephemeral env pytest + Docker, Robot Framework, contract tests E2E Validate user workflows end-to-end UI + backend + integrations Staging/near-prod Playwright, Cypress, Robot Framework"},{"location":"engineering-handbook/development-processes-and-automation/#recommended-expectations-per-tier","title":"Recommended Expectations Per Tier","text":"Tier Should Be Should Avoid Key Checks Unit Fast, deterministic, isolated; uses mocking/stubbing External dependencies (DB/network/filesystem) Normal and edge cases; stable assertions Integration Covers interfaces/config; uses realistic dependencies (often containerized) Excessive UI coverage; flakiness without stabilization API contracts, DB migrations, auth flows, config correctness E2E Focused on critical journeys; runs against deployed system Large brittle suites; duplicating lower-tier assertions Workflow success, cross-component behavior, basic UI accessibility"},{"location":"engineering-handbook/development-processes-and-automation/#cicd-execution-and-reporting","title":"CI/CD Execution and Reporting","text":"Area Guideline Automation Run unit tests on every PR; integration tests on PR and/or merge; E2E on merge to main and scheduled (nightly) where appropriate Quality gates Treat failures as release blockers for critical services; prioritize fixing flaky tests Reporting Provide a short CI summary (what failed + links) and expose detailed logs/reports via artifacts or dashboards for non-technical stakeholders Tooling Prefer standard frameworks that integrate with CI (e.g., pytest, Robot Framework, Playwright; k6 for performance) <p>!!! tip \"Test Pyramid\" Follow the test pyramid principle: many fast unit tests at the base, fewer integration tests in the middle, and a small number of E2E tests at the top. This ensures quick feedback while maintaining comprehensive coverage.</p>"},{"location":"engineering-handbook/development-processes-and-automation/#continuous-integration-continuous-deployment-cicd","title":"Continuous Integration / Continuous Deployment (CI/CD)","text":"<p>All changes should flow through CI/CD pipelines to ensure consistent quality and rapid feedback.</p>"},{"location":"engineering-handbook/development-processes-and-automation/#cicd-fundamentals","title":"CI/CD Fundamentals","text":"<ul> <li>CI (Continuous Integration): Every commit triggers automatic builds and tests</li> <li>CD (Continuous Deployment): Code that passes checks can be deployed to staging (and potentially production), depending on policy</li> </ul>"},{"location":"engineering-handbook/development-processes-and-automation/#baseline-pipeline-stages","title":"Baseline Pipeline Stages","text":"<p>A typical CI/CD pipeline includes the following stages:</p> <ol> <li>Code checkout and dependency resolution to prepare the build environment</li> <li>Static checks including linting and security scans</li> <li>Test execution running unit, integration, and E2E tests as defined</li> <li>Packaging and versioning of build artifacts</li> <li>Deployment to test/staging environments (optional, based on policy)</li> </ol> <p>!!! note \"Tool Flexibility\" Tools may vary by team (e.g., GitHub Actions, GitLab CI, Jenkins), but the goal is consistent: reduce manual intervention and provide fast feedback on every change.</p>"},{"location":"engineering-handbook/development-processes-and-automation/#benefits-of-cicd","title":"Benefits of CI/CD","text":"<ul> <li>Fast feedback \u2014 Detect breakages quickly, often within minutes</li> <li>Reduced integration problems \u2014 Smaller, frequent merges prevent merge conflicts</li> <li>Automation of repetitive tasks \u2014 Consistent builds, tests, and deployments</li> <li>Improved code quality \u2014 Automated checks enforce standards before merge</li> </ul> <p>!!! tip \"Pipeline Optimization\" Keep pipelines fast (ideally under 10 minutes for unit tests) to maintain developer productivity. Use caching, parallelization, and selective test execution where appropriate.</p>"},{"location":"engineering-handbook/documentation-standards-and-codification/","title":"Software Documentation Standards and Codification","text":"<p>In a multi-partner environment with mixed technical backgrounds, clear documentation is a prerequisite for maintainability, reuse, and safe collaboration. Documentation is not only for end users, but also for future contributors and maintainers. In ECHOES, documentation is treated as part of the development lifecycle and should evolve alongside the code.</p> <p>!!! note \"Normative Boundary\" This page is implementation guidance (Living Documentation). It does not create new normative requirements beyond those defined in D6.2 Chapter 8 and validated via Chapters 9\u201311.</p>"},{"location":"engineering-handbook/documentation-standards-and-codification/#documenting-apis-and-services","title":"Documenting APIs and Services","text":"<p>Every software component, API, or tool should include up-to-date documentation that explains:</p> <ul> <li>What it does \u2014 Purpose and scope</li> <li>How to install and deploy it \u2014 Setup and deployment procedures</li> <li>How to use it \u2014 Interfaces, inputs/outputs, and examples</li> <li>How to contribute or extend it \u2014 Development workflow and conventions</li> </ul>"},{"location":"engineering-handbook/documentation-standards-and-codification/#api-documentation-machine-readable","title":"API Documentation (Machine-Readable)","text":"<p>For APIs, prefer standard machine-readable formats:</p> <ul> <li>OpenAPI (Swagger) for REST APIs (YAML/JSON)</li> </ul>"},{"location":"engineering-handbook/documentation-standards-and-codification/#benefits-of-machine-readable-documentation","title":"Benefits of Machine-Readable Documentation","text":"<ul> <li>Enables client generation \u2014 Automatically generate SDKs and client libraries</li> <li>Supports interactive explorers \u2014 Tools like Swagger UI for testing</li> <li>Provides a testable contract \u2014 Automated validation against specification</li> <li>Improves consistency \u2014 Single source of truth for API behavior</li> </ul>"},{"location":"engineering-handbook/documentation-standards-and-codification/#documentation-should-live-with-the-code","title":"Documentation Should Live with the Code","text":"<p>Documentation should be maintained in the same repository as the component and versioned together with releases. This ensures:</p> <ul> <li>The docs match the deployed version</li> <li>Changes are reviewable in PRs</li> <li>Documentation evolves with the implementation</li> <li>No drift between code and documentation</li> </ul>"},{"location":"engineering-handbook/documentation-standards-and-codification/#best-practices-for-workflow-documentation","title":"Best Practices for Workflow Documentation","text":"<p>When explaining non-trivial behavior (e.g., metadata harvesting, validation pipelines):</p> <ul> <li>Use diagrams where they improve comprehension</li> <li>Keep language direct and practical \u2014 Avoid unnecessary jargon</li> <li>Optimize for onboarding \u2014 Write for a new developer or institution adopting the component</li> </ul>"},{"location":"engineering-handbook/documentation-standards-and-codification/#api-documentation-checklist","title":"API Documentation Checklist","text":"<p>Good API documentation should include the following elements:</p> Element Description Overview What the API does and who it is for Authentication How to authenticate and authorize requests Endpoints Resources and operations with clear descriptions Request/response examples Sample calls and expected responses Error model Error codes, meanings, and handling guidance Constraints Rate limits, pagination, filtering, payload limits Versioning Version being documented and compatibility notes Changelog What changed between versions (especially breaking changes)"},{"location":"engineering-handbook/documentation-standards-and-codification/#documentation-tooling-recommendations","title":"Documentation Tooling Recommendations","text":"Tool Best For Format OpenAPI/Swagger REST APIs YAML / JSON Sphinx Python projects, technical docs reStructuredText MkDocs General documentation Markdown Docusaurus Project sites, versioned docs Markdown / React Read the Docs Hosting + automated doc builds Multiple formats <p>!!! tip \"Choose the Right Tool\" Select documentation tools that integrate well with your existing development workflow and support versioning alongside code releases.</p>"},{"location":"engineering-handbook/documentation-standards-and-codification/#infrastructure-as-code-iac-and-process-codification","title":"Infrastructure as Code (IaC) and Process Codification","text":"<p>ECHOES treats not only infrastructure but also operational processes as code. This means writing down how services are deployed, configured, and maintained in artifacts that are:</p> <ul> <li>Stored in version control for history and auditability</li> <li>Reviewable via PR/MR for quality assurance</li> <li>Reproducible and automatable for consistency</li> </ul>"},{"location":"engineering-handbook/documentation-standards-and-codification/#what-to-codify","title":"What to Codify","text":"<p>Examples of processes and infrastructure that should be codified:</p> <ul> <li>Ansible playbooks defining provisioning steps</li> <li>Dockerfiles and Compose/Kubernetes manifests defining runtime behavior</li> <li>CI/CD pipeline definitions (e.g., <code>.gitlab-ci.yml</code>, GitHub Actions workflows)</li> <li>Scripts and configuration for automated testing and monitoring setup</li> </ul>"},{"location":"engineering-handbook/documentation-standards-and-codification/#benefits-of-codification","title":"Benefits of Codification","text":"Benefit Impact Reproducibility Environments can be recreated consistently across teams Auditability Changes are tracked and reviewed through version control Documentation-by-default The code describes how the system works Automation Codified processes can be triggered and validated automatically Disaster recovery Rebuild infrastructure quickly from source-controlled artifacts <p>!!! tip \"Infrastructure as Documentation\" Well-written Infrastructure as Code serves as both executable configuration and living documentation of your system architecture.</p>"},{"location":"engineering-handbook/documentation-standards-and-codification/#example-ansible-playbook","title":"Example: Ansible Playbook","text":"<p>Here's an illustrative example of deploying a service using Ansible:</p> <pre><code>- name: Deploy ECHOES metadata service\n  hosts: metadata_servers\n  become: yes\n\n  tasks:\n    - name: Install Docker\n      apt:\n        name: docker.io\n        state: present\n        update_cache: yes\n\n    - name: Pull service container\n      docker_image:\n        name: echoes/metadata-service\n        tag: v2.1.0\n        source: pull\n\n    - name: Start service\n      docker_container:\n        name: metadata-service\n        image: echoes/metadata-service:v2.1.0\n        state: started\n        ports:\n          - \"8080:8080\"\n        env:\n          DB_HOST: \"postgres.echoes.local\"\n          LOG_LEVEL: \"info\"\n</code></pre>"},{"location":"engineering-handbook/documentation-standards-and-codification/#iac-best-practices","title":"IaC Best Practices","text":"<ul> <li>Version everything \u2014 Keep all infrastructure definitions in Git</li> <li>Use meaningful names \u2014 Make playbooks and scripts self-documenting</li> <li>Modularize configurations \u2014 Break complex deployments into reusable components</li> <li>Test infrastructure changes \u2014 Validate in non-production environments first</li> <li>Document non-obvious decisions \u2014 Use comments to explain why, not just what</li> </ul>"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/","title":"Security, Data Protection, and Secrets Privacy","text":"<p>Traceability to requirements: This section provides implementation guidance supporting the canonical security and operability requirements in D6.2 Chapter 8 (e.g., REQ-SEC-* and related deployment/logging requirements such as REQ-DEP-011 on log redaction). Guidance here does not replace normative requirement statements.</p> <p>Secure development and deployment practices are essential when components handle sensitive cultural data, connect across institutional boundaries, and operate in public environments. This is not only about protecting software, but also about protecting partner trust and sustaining the CH Cloud as a reliable ecosystem.</p> <p>!!! note \"Boundary to \u00a73.4\" Chapter 3.4 describes the cross-component trust framework (AAI integration, authn/authz models, API transport security, privacy practices). This chapter focuses on implementation practices for secure development, secrets handling, security testing, and incident/update processes.</p>"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#secure-secrets-management","title":"Secure Secrets Management","text":"<p>Secrets (API keys, database passwords, encryption keys, service tokens) must never be hardcoded or committed to version control. They should be managed through dedicated mechanisms and injected at runtime.</p>"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#why-secrets-management-matters","title":"Why Secrets Management Matters","text":"Reason Benefit Security Reduces blast radius if code/repositories leak Rotation Supports regular credential changes without code updates Auditability Enables \"who accessed what, when\" tracking Separation of concerns Developers do not require production credentials"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#recommended-approaches","title":"Recommended Approaches","text":"<p>Choose the appropriate secrets management approach based on your deployment environment:</p> Approach Description Best For Environment variables Inject secrets as env vars at runtime Simple deployments, containers HashiCorp Vault Central secrets management with access control Production environments, multiple services Cloud provider secrets AWS Secrets Manager / GCP Secret Manager / Azure Key Vault Cloud-native deployments Kubernetes Secrets K8s built-in secret primitives (prefer KMS-backed when possible) Orchestrated services"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#secrets-in-cicd-pipelines","title":"Secrets in CI/CD Pipelines","text":"<p>Most CI/CD platforms provide secure secret injection mechanisms:</p> <ul> <li>GitHub Actions: Repository/environment secrets</li> <li>GitLab CI: Masked/protected variables</li> <li>Jenkins: Credentials plugin</li> </ul>"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#operational-rules-for-cicd-secrets","title":"Operational Rules for CI/CD Secrets","text":"<ul> <li>Mark secrets as masked to prevent log exposure</li> <li>Avoid printing environment variables in pipeline logs</li> <li>Scope secrets to environments and apply least privilege principles</li> <li>Rotate secrets regularly and audit access patterns</li> </ul> <p>!!! warning \"Never Commit Secrets\" Use pre-commit hooks and secret scanning tools to prevent accidental commits of credentials to version control.</p>"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#continuous-security-testing-and-compliance-checks","title":"Continuous Security Testing and Compliance Checks","text":"<p>Security is continuous, not a one-time effort. Automated checks should be integrated into pipelines to detect issues early and enforce baseline standards.</p>"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#security-testing-categories","title":"Security Testing Categories","text":"Category What It Checks Example Tools SAST (Static Application Security Testing) Insecure patterns in source code Bandit, SonarQube, Semgrep Dependency scanning (SCA) Known vulnerable libraries Dependabot, Snyk, OWASP Dependency-Check, Renovate Container scanning Image vulnerabilities Trivy, Clair, Grype DAST (Dynamic Application Security Testing) Runtime/web vulnerabilities OWASP ZAP, Burp Suite Secret scanning Committed credentials TruffleHog, GitGuardian, git-secrets"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#dependency-management","title":"Dependency Management","text":"<p>Practical guidance for managing dependencies securely:</p> <ul> <li>Keep dependencies updated using automated PRs via Dependabot or Renovate</li> <li>Monitor security advisories for your dependency ecosystem</li> <li>Pin major versions but allow patch/minor updates where safe</li> <li>Use lock files for reproducible builds (<code>package-lock.json</code>, <code>poetry.lock</code>, <code>Pipfile.lock</code>, etc.)</li> <li>Review dependency licenses for compliance with project requirements</li> </ul>"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#container-security","title":"Container Security","text":"<p>If using containers, follow these security practices:</p> <ul> <li>Prefer minimal official base images (e.g., <code>python:3.10-slim</code>, <code>alpine</code>)</li> <li>Run as non-root where feasible to limit potential damage</li> <li>Scan images regularly and rebuild/patch frequently</li> <li>Remove unnecessary packages and tools to reduce attack surface</li> <li>Use multi-stage builds to separate build dependencies from runtime</li> <li>Sign and verify images for supply chain security</li> </ul>"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#compliance-and-policy-enforcement","title":"Compliance and Policy Enforcement","text":"<p>Use policy-as-code tools to enforce security and compliance controls automatically:</p> Tool Purpose Best For OPA (Open Policy Agent) Policy enforcement across stack General-purpose policy decisions Checkov IaC security scanning Terraform, CloudFormation, Kubernetes manifests Kyverno Kubernetes-native policy management K8s admission control and validation <p>!!! tip \"Shift Left Security\" Integrate security checks early in the development lifecycle to catch issues before they reach production. Automated security gates in CI/CD pipelines make this practical and sustainable.</p>"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#incident-management-and-secure-update-policies","title":"Incident Management and Secure Update Policies","text":"<p>Even with strong prevention measures, incidents can occur. A defined response process minimizes impact and maintains trust across the consortium.</p>"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#incident-response-lifecycle","title":"Incident Response Lifecycle","text":"<p>Follow this structured approach to security incidents:</p> Phase Actions Key Outcomes 1. Detection &amp; Reporting Monitor alerts/logs; provide reporting channels; acknowledge reports promptly Incident identified and logged 2. Assessment Determine severity and scope; identify affected systems/data; classify impact (e.g., CVSS) Impact understood and prioritized 3. Containment Isolate systems; revoke credentials; block malicious traffic Threat contained and spread prevented 4. Remediation Patch/fix vulnerabilities; update configuration; deploy new versions Root cause addressed 5. Communication Notify affected partners/users; be factual; document lessons learned Stakeholders informed transparently 6. Post-incident Review Root cause analysis; update controls; improve detection/response Future incidents prevented"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#vulnerability-disclosure-policy","title":"Vulnerability Disclosure Policy","text":"<p>Establish clear expectations for responsible disclosure:</p> <ul> <li>Define disclosure timeline (e.g., 90 days for non-critical issues)</li> <li>Provide contact point (security email or reporting path)</li> <li>Clarify acknowledgement/credit policy for security researchers</li> <li>Coordinate remediation before public disclosure when appropriate</li> <li>Maintain a security advisory page for historical reference</li> </ul>"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#security-update-policy","title":"Security Update Policy","text":"<p>Manage security updates with appropriate urgency and process:</p>"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#prioritization-by-severity","title":"Prioritization by Severity","text":"<ul> <li>Critical vulnerabilities: Patch within 24-48 hours</li> <li>High severity: Patch within 1 week</li> <li>Medium severity: Include in next regular release cycle</li> <li>Low severity: Address during planned maintenance</li> </ul>"},{"location":"engineering-handbook/security-secrets-and-secure-sdlc/#update-best-practices","title":"Update Best Practices","text":"<ul> <li>Test patches even under time pressure to avoid introducing regressions</li> <li>Communicate changes and impact to affected stakeholders</li> <li>Provide rollback procedures in case of issues</li> <li>Retain records of incidents, decisions, and responses for auditability</li> <li>Document lessons learned to improve future response</li> </ul> <p>!!! tip \"Security Communication\" Be transparent about security issues while avoiding disclosure of exploit details until patches are widely deployed. Balance transparency with responsible disclosure to protect users.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/","title":"Service Management and Interoperability Standards","text":"<p>Beyond writing code, ECHOES components must be managed as services that are maintained, monitored, and evolved over time. This page provides practical guidance on adopting lightweight service management frameworks, managing dependencies and APIs for reliability and interoperability, and using event-driven architecture and standardized messaging patterns for loosely coupled workflows.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#adopting-service-management-frameworks","title":"Adopting Service Management Frameworks","text":"<p>Service management frameworks provide structured approaches to delivering and maintaining IT services. For research projects, full ITIL adoption may be unnecessarily heavyweight. Lightweight alternatives such as FitSM (tailored for research and education environments) can provide practical structure with lower overhead.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#core-service-management-principles","title":"Core Service Management Principles","text":"Principle What It Means in Practice Typical Outputs / Artifacts Service catalog Maintain an inventory of available services and their purpose Service catalog page; service owners; service boundaries Service level objectives (SLOs) Define expected availability and performance targets SLO definitions; monitoring dashboards; alert thresholds Change management Control and document how changes are introduced to reduce disruption Change log; approval rules; release notes; rollback plan Incident management Handle incidents consistently and traceably Incident process; escalation paths; post-incident reviews Continuous improvement Regularly review service quality and implement improvements Improvement backlog; retrospectives; user feedback loop"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#fitsm-for-research-projects","title":"FitSM for Research Projects","text":"<p>FitSM is a lightweight service management standard designed specifically for research and education IT services. It provides a practical structure across the service lifecycle:</p> FitSM Area Purpose Example Evidence (Lightweight) Service planning Define what services will be delivered Service roadmap; scope statement; resourcing assumptions Service design Design services to meet requirements Architecture notes; security considerations; SLO draft Service delivery Operate and maintain services Runbooks; deployment procedures; on-call/support rotation Service assurance Monitor and improve service quality Monitoring/alerts; incident metrics; periodic review notes"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#application-to-echoes","title":"Application to ECHOES","text":"Practice What to Do Outcome Document service descriptions and responsibilities Assign service owner(s); define service boundaries; publish a short description Clear accountability; easier onboarding and support Define support procedures Provide intake channels, triage rules, escalation, response expectations Predictable user experience; faster resolution Establish monitoring and reporting Define basic metrics and alerting; periodic status reporting Improved reliability; earlier detection of issues Create improvement cycles based on user feedback Capture feedback; maintain improvement backlog; review periodically Continuous service maturity; better alignment with users"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#minimum-documentation-requirements","title":"Minimum Documentation Requirements","text":"<p>Each service should have a minimum documentation set to ensure operability and supportability:</p> Document Element Required Content Notes / Examples Service description What it does; who it is for; key features; service owner Keep to 0.5\u20131 page; link to user docs Operational procedures How to deploy, update, restart, scale; dependencies Include step-by-step runbook and rollback Support contacts Who to contact; hours; escalation path Include on-call rotation if applicable SLOs/SLAs Availability target; performance targets; support response targets Prefer SLOs; SLAs only if contractually needed Known issues and workarounds Current limitations; mitigations; planned fixes Keep current; link to issue tracker Maintenance windows When updates occur; expected impact; notification approach Define communication template and lead time <p>!!! tip \"Keep It Lightweight\" Documentation should be sufficient for operation and handoff, but avoid creating bureaucracy. A well-maintained README, runbook, and service catalog entry are often sufficient for research projects.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#managing-dependencies-and-apis-for-interoperability","title":"Managing Dependencies and APIs for Interoperability","text":"<p>Cultural heritage cloud services rarely operate in isolation. They depend on external APIs, databases, authentication services, and other components. Managing these dependencies carefully is essential for reliability and interoperability.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#dependency-management-strategies","title":"Dependency Management Strategies","text":""},{"location":"engineering-handbook/service-management-and-interoperability-standards/#version-pinning-and-compatibility","title":"Version Pinning and Compatibility","text":"<p>Dependencies should be managed with explicit version constraints to ensure predictable behavior across environments.</p> <p>Key practices:</p> <ul> <li>Specify exact or minimum versions for all dependencies</li> <li>Use lock files to freeze the entire dependency tree</li> <li>Test compatibility in non-production environments before upgrading</li> <li>Document and communicate breaking changes clearly</li> </ul> <p>!!! info \"Why It Matters\" Version pinning prevents unexpected breakages when libraries update and ensures all environments use identical versions.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#graceful-degradation","title":"Graceful Degradation","text":"<p>Services should remain partially functional when dependencies become unavailable rather than failing completely.</p> <p>Implementation approach:</p> <ul> <li>Provide meaningful error messages explaining what is affected and why</li> <li>Offer degraded functionality where possible (cached data, static content)</li> <li>Log all dependency failures for investigation</li> <li>Design to prevent cascading failures</li> </ul> <p>!!! info \"Why It Matters\" One service's failure should not bring down the entire system; users get partial functionality instead of total outage.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#circuit-breakers","title":"Circuit Breakers","text":"<p>Circuit breakers prevent repeated calls to failing services, protecting both the caller and the failing dependency.</p> <p>How they work:</p> <ul> <li>Closed state: Normal operation, requests pass through</li> <li>Open state: After repeated failures, immediately return errors without attempting calls</li> <li>Half-open state: After timeout, allow test requests to check if service recovered; return to closed if successful, reopen if it fails</li> </ul> <p>!!! info \"Why It Matters\" Prevents wasted resources on calls that will fail and gives failing services time to recover.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#timeouts-and-retries","title":"Timeouts and Retries","text":"<p>Every external call needs timeout and retry policies to handle both transient and permanent failures.</p> <p>Configuration principles:</p> <ul> <li>Set timeouts based on expected response times plus network margin</li> <li>Implement retry with exponential backoff for transient failures</li> <li>Do not retry permanent errors (authentication failures, invalid requests)</li> <li>Limit retry attempts and add jitter to prevent synchronized retry storms</li> </ul> <p>!!! info \"Why It Matters\" Prevents requests from hanging indefinitely while handling temporary network issues appropriately.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#monitoring-dependencies","title":"Monitoring Dependencies","text":"<p>Track dependency health proactively to catch problems early.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#what-to-monitor","title":"What to Monitor","text":"<ul> <li>Response times \u2014 Increasing times signal problems before failures</li> <li>Error rates \u2014 Rising errors indicate degraded service quality</li> <li>Availability patterns \u2014 Identify recurring issues</li> </ul>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#alerting-strategy","title":"Alerting Strategy","text":"<ul> <li>Set thresholds based on error rates and response time percentiles</li> <li>Provide context: which dependency, severity, affected services</li> <li>Escalate critical dependencies faster than optional ones</li> </ul>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#dependency-mapping","title":"Dependency Mapping","text":"<ul> <li>Document which services depend on which others</li> <li>Identify critical paths where failures have cascading impact</li> <li>Use maps to prioritize reliability improvements</li> </ul>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#fallback-plans","title":"Fallback Plans","text":"<ul> <li>Document alternatives for critical dependencies</li> <li>Options: backup services, cached data, degraded functionality</li> <li>Test fallback procedures regularly to ensure they work when needed</li> </ul>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#event-driven-architecture-and-standardized-messaging-protocols","title":"Event-Driven Architecture and Standardized Messaging Protocols","text":"<p>For complex workflows and loosely coupled systems, event-driven architecture provides flexibility and scalability. Instead of services calling each other directly, they emit and respond to events.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#benefits-of-event-driven-architecture","title":"Benefits of Event-Driven Architecture","text":"Benefit Description Impact Loose coupling Services do not need to know about each other's existence or location Easier to add, remove, or modify services independently Scalability Events processed asynchronously, allowing independent scaling Handle traffic spikes by adding more event consumers Resilience Temporary failures do not block the entire workflow System continues operating even when individual services fail Auditability Event logs provide complete history of all changes Full traceability for compliance and debugging"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#messaging-patterns","title":"Messaging Patterns","text":""},{"location":"engineering-handbook/service-management-and-interoperability-standards/#publishsubscribe","title":"Publish\u2013Subscribe","text":"<p>Services publish events to topics; interested services subscribe.</p> <p></p> <p>When to use:</p> <ul> <li>Multiple services need to react to the same event</li> <li>Services should remain unaware of each other</li> <li>New subscribers can be added without modifying publishers</li> </ul> <p>Example scenario: When a cultural heritage object is updated, the indexing service updates the search index, the notification service alerts curators, and the analytics service records the change\u2014without the metadata service needing to know about these consumers.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#message-queues","title":"Message Queues","text":"<p>Work items are placed in queues and processed by workers.</p> <p></p> <p>When to use:</p> <ul> <li>Tasks need guaranteed processing (at-least-once delivery)</li> <li>Work should be distributed across multiple workers</li> <li>Processing can be asynchronous (not real-time)</li> </ul> <p>Example scenario: Digitization pipeline where image processing tasks are queued and distributed across multiple worker nodes for parallel processing.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#event-sourcing","title":"Event Sourcing","text":"<p>All changes are stored as a sequence of events rather than just current state, providing complete audit trail and ability to reconstruct state at any point in time.</p> <p>Example event store:</p> <ul> <li>Event 1: Object created: <code>obj-12345</code></li> <li>Event 2: Title updated: <code>\"Medieval Manuscript\"</code></li> <li>Event 3: Metadata enriched: added date field</li> <li>Event 4: Access rights changed: public \u2192 restricted</li> <li>Event 5: Annotation added by curator</li> </ul> <p>When to use:</p> <ul> <li>Complete audit history is required</li> <li>Need to reconstruct past states</li> <li>Temporal queries (\"what did this object look like in 2023?\")</li> <li>Compliance and provenance tracking</li> </ul> <p>!!! info \"Why It Matters for Cultural Heritage\" Tracking the complete history of object metadata changes is often essential for scholarly research and institutional accountability.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#recommended-technologies","title":"Recommended Technologies","text":"Technology Use Case Protocol RabbitMQ General message queuing AMQP Apache Kafka High-throughput event streaming Kafka protocol Redis Pub/Sub Lightweight messaging Redis protocol MQTT IoT and lightweight messaging MQTT"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#event-schema-standards","title":"Event Schema Standards","text":"<p>Define clear, consistent schemas for all events to ensure interoperability.</p>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#required-fields-for-all-events","title":"Required Fields for All Events","text":"<ul> <li><code>event_id</code>: Unique identifier for this event</li> <li><code>event_type</code>: Categorized event name (e.g., <code>object.created</code>, <code>metadata.updated</code>)</li> <li><code>timestamp</code>: When the event occurred (ISO 8601 format)</li> <li><code>source</code>: Which service generated the event</li> <li><code>data</code>: Event-specific payload</li> </ul>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#example-event-structure","title":"Example Event Structure","text":"<pre><code>{\n  \"event_id\": \"evt-abc123\",\n  \"event_type\": \"object.created\",\n  \"timestamp\": \"2024-01-15T14:30:00Z\",\n  \"source\": \"metadata-service\",\n  \"data\": {\n    \"object_id\": \"obj-12345\",\n    \"collection\": \"medieval-manuscripts\",\n    \"metadata_url\": \"https://api.echoes.eu/objects/obj-12345\"\n  }\n}\n</code></pre>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#event-driven-architecture-best-practices","title":"Event-Driven Architecture Best Practices","text":""},{"location":"engineering-handbook/service-management-and-interoperability-standards/#schema-governance","title":"Schema Governance","text":"<ul> <li>Document all event types in a shared registry to maintain consistency across services</li> <li>Version event schemas when making breaking changes to prevent consumer disruption</li> <li>Use semantic event names following the <code>verb.noun</code> pattern (e.g., <code>object.created</code>, <code>metadata.updated</code>)</li> <li>Include correlation IDs to trace related events across services and enable end-to-end request tracking</li> </ul>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#event-design-best-practices","title":"Event Design Best Practices","text":"<ul> <li>Keep events small and focused following the single responsibility principle</li> <li>Include sufficient information for consumers to act without requiring additional queries to other services</li> <li>Avoid including sensitive data in events that are broadcast to multiple subscribers</li> <li>Use standard timestamp formats (ISO 8601) and timezone (UTC) for consistency</li> </ul>"},{"location":"engineering-handbook/service-management-and-interoperability-standards/#implementation-considerations","title":"Implementation Considerations","text":"<p>Ordering and Idempotency</p> <p>Events may arrive out of order or be delivered multiple times in distributed systems. To handle this:</p> <ul> <li>Design consumers to be idempotent so that processing the same event twice produces the same result</li> <li>Use sequence numbers or timestamps when event order is critical to business logic</li> </ul> <p>Error Handling</p> <p>Implement robust error handling to ensure reliability:</p> <ul> <li>Ensure failed event processing does not lose messages through proper retry mechanisms</li> <li>Implement dead-letter queues for events that repeatedly fail processing after exhausting retries</li> <li>Log processing failures with sufficient context including event details, error messages, and stack traces for effective debugging</li> </ul> <p>Monitoring</p> <p>Establish comprehensive monitoring to maintain system health:</p> <ul> <li>Track event publication rates and consumption lag to identify performance issues</li> <li>Alert on growing backlogs when consumers are falling behind producers</li> <li>Monitor dead-letter queues for systematic processing failures that may indicate code bugs or infrastructure problems</li> </ul> <p>!!! tip \"Start Simple\" Begin with basic pub-sub patterns before implementing complex event sourcing. Event-driven architecture can be adopted incrementally as your needs grow.</p>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/","title":"Test Automation and Quality Assurance","text":"<p>Comprehensive test automation ensures that changes do not break existing functionality and that quality remains high throughout the development lifecycle. This section covers test execution orchestration, framework selection, and reporting/analytics.</p> <p>!!! note \"Normative Boundary\" This is implementation guidance (Living Documentation). The canonical interoperability requirements are defined in D6.2 Chapter 8 and validated via Chapters 9\u201311.</p>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#automated-test-execution-and-orchestration","title":"Automated Test Execution and Orchestration","text":"<p>Test automation is not only about writing tests. It requires orchestrating execution across environments, managing test data, and scaling execution via parallelization while keeping results reliable.</p>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#test-levels-and-execution-order-fail-fast","title":"Test Levels and Execution Order (Fail Fast)","text":"<p>Tests should be executed in order of speed to provide fast feedback:</p> <pre><code>[Fast tests first] \u2192 [Slower tests] \u2192 [E2E tests]\n</code></pre> <p>Recommended execution order:</p> <ol> <li>Unit tests (seconds)</li> <li>Integration tests (minutes)</li> <li>End-to-end (E2E) tests (minutes to hours)</li> </ol>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#rationale","title":"Rationale","text":"<ul> <li>Fast feedback loop \u2014 Unit tests catch many issues quickly</li> <li>Fail fast \u2014 Stop early if fundamentals fail</li> <li>Resource efficiency \u2014 Don't spend CI time on slow suites if basics are broken</li> <li>Developer experience \u2014 Quick local testing without waiting for full pipelines</li> </ul>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#parallel-execution","title":"Parallel Execution","text":"<p>Independent tests should run simultaneously to reduce total runtime.</p>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#benefits","title":"Benefits","text":"<ul> <li>Reduces CI/CD pipeline duration significantly</li> <li>Enables frequent testing without large time penalties</li> <li>Better utilization of compute resources</li> <li>Maintains fast feedback as the suite grows</li> </ul>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#considerations","title":"Considerations","text":"<ul> <li>Tests must be independent \u2014 No shared state assumptions</li> <li>Integration tests involving databases may require isolated schemas or dedicated containers</li> <li>Test data must be controlled to avoid conflicts between parallel runs</li> <li>Balance parallelism against available runner resources</li> </ul> <p>!!! tip \"Optimize for Speed\" A test suite that runs in 5 minutes instead of 30 minutes dramatically improves developer productivity and enables more frequent testing.</p>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#test-environment-management","title":"Test Environment Management","text":"<p>Consistent environments reduce \"works on my machine\" failures and flaky results.</p>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#recommended-approaches","title":"Recommended Approaches","text":"Approach Description Benefits Containers Isolated, reproducible environments per run Complete isolation; version control of dependencies Virtual environments Language-level dependency isolation Lightweight; fast setup Infrastructure as Code Define test infrastructure programmatically Reproducible; auditable; version-controlled Ephemeral test databases Fresh instances per suite (or per parallel shard) No state pollution; true isolation <p>!!! info \"Why It Matters\" Inconsistent environments cause flaky tests and unreliable results. Containerization and IaC help ensure identical execution across developer machines and CI runners.</p>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#using-test-frameworks","title":"Using Test Frameworks","text":"<p>Different testing needs require different tools. Projects should select frameworks based on what is being tested and who maintains the tests.</p>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#framework-selection-guide","title":"Framework Selection Guide","text":"Framework Type Best For Key Strengths Robot Framework Keyword-driven Acceptance testing; mixed technical teams Human-readable; rich library ecosystem; HTML reports k6 Load/performance Performance and scalability validation JS scripting; realistic load; CI-friendly metrics Playwright E2E Web UI workflows Cross-browser; auto-wait; trace/debug tooling pytest Unit/integration Python services Fixtures; parametrization; plugin ecosystem JUnit Unit/integration Java services Industry standard; IDE support Jest Unit/integration JS/TS services Fast; snapshot testing; built-in mocking"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#robot-framework-acceptance-testing","title":"Robot Framework (Acceptance Testing)","text":"<p>When to use:</p> <ul> <li>Acceptance tests that non-developers should understand</li> <li>API tests expressed as clear test cases</li> <li>Teams with mixed technical backgrounds</li> </ul> <p>Key characteristics:</p> <ul> <li>Keyword-driven syntax readable by non-programmers</li> <li>Libraries for web/API/DB testing</li> <li>Generates detailed HTML reports</li> </ul> <p>Example use case: Validating metadata API endpoints with cases that domain experts can review.</p>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#k6-loadperformance-testing","title":"k6 (Load/Performance Testing)","text":"<p>When to use:</p> <ul> <li>Validating performance under load</li> <li>Scalability testing before production promotion</li> <li>Identifying bottlenecks and capacity limits</li> </ul> <p>Key characteristics:</p> <ul> <li>JavaScript test scripts for familiar syntax</li> <li>Realistic user behavior modeling</li> <li>Detailed performance metrics</li> <li>Native CI/CD integration</li> </ul> <p>Example use case: Simulating 1000 concurrent users querying the metadata catalogue and verifying response-time targets.</p>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#playwright-e2e-web-ui","title":"Playwright (E2E Web UI)","text":"<p>When to use:</p> <ul> <li>Testing web UI workflows</li> <li>Validating end-to-end user journeys</li> <li>Cross-browser compatibility verification</li> </ul> <p>Key characteristics:</p> <ul> <li>Stable automation with reduced flakiness</li> <li>Auto-waiting (less need for manual delays)</li> <li>Chromium/Firefox/WebKit support</li> <li>Advanced debugging tools (trace viewer)</li> </ul> <p>Example use case: Curator login \u2192 search \u2192 view object \u2192 add annotation workflow validation.</p>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#results-reporting-and-analytics","title":"Results Reporting and Analytics","text":"<p>Testing does not end at execution. Effective QA includes reporting, trend analysis, and integration with team workflows.</p>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#what-effective-reports-should-provide","title":"What Effective Reports Should Provide","text":"Report Element Purpose Audience Overall pass/fail status Quick health check Everyone Detailed failure information Debugging (logs, traces, screenshots) Developers Execution trends Detect degradation over time Technical leads Coverage metrics Assess completeness QA / engineering leads Performance benchmarks Track performance regression Operations / platform owners"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#tools-for-reporting","title":"Tools for Reporting","text":"Tool Features Best For Allure Rich HTML reports, history, trend analysis Detailed analysis with low setup overhead ReportPortal Dashboards, failure clustering/analysis Large test suites and pattern detection Codecov Coverage tracking, diff coverage, trends Coverage governance and regression prevention TestRail Test case management, planning, traceability Formal test planning and traceability"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#selection-guidance","title":"Selection Guidance","text":"<ul> <li>Allure: Best for high-quality reports with minimal overhead</li> <li>ReportPortal: Ideal when suite scale requires deeper analytics</li> <li>Codecov: Essential for continuous coverage governance</li> <li>TestRail: Useful when formal QA planning and traceability are needed</li> </ul>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#test-analytics-and-trends","title":"Test Analytics and Trends","text":"<p>Monitor these metrics to maintain test suite health:</p> Metric What It Reveals Typical Actions Test stability Intermittent failures (flaky tests) Fix or quarantine; remove nondeterminism Execution time trends Suite getting slower Optimize slow tests; improve infrastructure; parallelize Coverage trends Coverage increasing/decreasing Enforce thresholds; focus on critical paths Failure patterns Recurring weak areas Improve tests and refactor fragile code"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#practical-guidance","title":"Practical Guidance","text":"<ul> <li>Flaky tests undermine trust and waste time \u2014 track and eliminate them systematically</li> <li>Optimize slow tests first \u2014 Often a small subset dominates runtime</li> <li>Use failure patterns to prioritize refactoring or deeper coverage where fragility is concentrated</li> </ul> <p>!!! warning \"Flaky Tests Are Technical Debt\" Flaky tests that fail intermittently erode confidence in the entire test suite. Treat them as high-priority technical debt and fix or remove them promptly.</p>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#event-driven-test-notifications","title":"Event-Driven Test Notifications","text":"<p>Integrating test outcomes into team communication improves time-to-awareness and reduces regressions reaching production. Notifications should be actionable, routed by severity, and designed to avoid alert fatigue.</p>"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#notification-model","title":"Notification Model","text":"Area Recommendation Purpose Scope CI pipeline tests, scheduled suites, performance benchmarks, coverage thresholds Visibility on quality gates Ownership Clear triage owner for critical failures; shared visibility for non-critical Fast response and accountability Principle Notify only when action is required; route by severity Avoid alert fatigue"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#notification-triggers","title":"Notification Triggers","text":"Trigger Typical Threshold Severity Suggestion CI/CD suite failure Any failed required stage High (blocks merge/deploy) Coverage drop Below threshold or significant drop vs baseline Medium\u2013High (policy-dependent) Performance regression Exceeds limit or regresses beyond tolerance Medium\u2013High (prod-risk = High) Critical-path failures Smoke/security/critical workflow failures High/Critical"},{"location":"engineering-handbook/test-automation-and-quality-assurance/#delivery-channels","title":"Delivery Channels","text":"Channel When to Use Content Expectation Slack/Teams Immediate CI failures Short summary + links to logs/runs Email digests Nightly/weekly suites Trends + top failures Dashboards Continuous visibility KPIs, history, aggregated status On-call escalation Critical failures with prod impact risk Pager-style escalation + runbook link <p>!!! tip \"Right Channel, Right Time\" Use synchronous channels (Slack/Teams) for urgent issues that require immediate action, and asynchronous channels (email, dashboards) for trends and regular reporting.</p>"},{"location":"standards-catalogue/data-architecture-standards-and-formats-3.1/","title":"Data architecture \u2013 standards and formats (from D6.2 \u00a73.1)","text":"<p>This page provides a technical overview of data standards and formats widely used in cultural heritage, digital humanities, and research infrastructures. It is intended as a living reference to support onboarding and integration in the ECHOES / CH Cloud.</p> <p>!!! note \"Normative boundary\" The authoritative interoperability requirements (including REQ tables referenced below) remain in the official deliverable (PDF). This page explains concepts, selection criteria, and practical guidance, and may evolve over time.</p>"},{"location":"standards-catalogue/data-architecture-standards-and-formats-3.1/#why-standards-matter-for-the-ch-cloud","title":"Why standards matter for the CH Cloud","text":"<p>The CH Cloud integrates datasets, tools, and services across institutions, disciplines, and countries. Without shared standards: - data cannot be combined or compared reliably, - search and discovery across collections becomes inconsistent, - tools and services cannot process heterogeneous inputs predictably, - integrators lose time on ad-hoc conversions and mappings, - preservation and sustainable access are harder to guarantee.</p> <p>Using established standards reduces integration friction, increases reuse, and supports scalability.</p>"},{"location":"standards-catalogue/data-architecture-standards-and-formats-3.1/#what-standards-cover","title":"What \u201cstandards\u201d cover","text":"<p>Data standards typically address three complementary layers:</p> <ul> <li>Syntax \u2013 how data is structured (fields, datatypes, file formats)</li> <li>Semantics \u2013 what the data means (concepts, relationships, vocabularies)</li> <li>Protocols \u2013 how data is exchanged (APIs, messages, serialisations)</li> </ul> <p>Together these layers enable interoperability across heterogeneous systems.</p>"},{"location":"standards-catalogue/data-architecture-standards-and-formats-3.1/#key-concepts-glossary","title":"Key concepts (glossary)","text":"<ul> <li>Metadata: \u201cdata about data.\u201d Example: for a photo of a painting\u2014artist, date, location, licence, resolution.</li> <li>Semantic interoperability: systems can exchange data and interpret it consistently (e.g., \u201ccreator\u201d in one system corresponds to \u201cartist\u201d in another).</li> <li>Serialisation: an encoding of the same information in different concrete representations (e.g., RDF in Turtle, RDF/XML, JSON-LD).</li> <li>Linked Data: a graph approach where resources are connected via explicit relationships and stable identifiers (URIs), forming a \u201cweb of knowledge\u201d rather than isolated records.</li> <li>URI (Uniform Resource Identifier): a persistent identifier for a resource (often HTTP-based) used consistently across datasets and systems.</li> <li>Ontology: a formal model describing domain concepts and their relationships (objects, events, places, actors, etc.).</li> </ul> <p>Maintenance note: when this wiki is connected to your standards catalogue, link \u201cLinked Data\u201d and \u201cURI\u201d to your project\u2019s preferred guidance pages (e.g., identifier policy, PID policy).</p>"},{"location":"standards-catalogue/data-architecture-standards-and-formats-3.1/#the-standards-landscape-categories","title":"The standards landscape (categories)","text":"<p>Cultural heritage standards can be grouped into several technical categories:</p> <ol> <li> <p>Semantic and knowledge representation    How meaning and relationships are expressed (e.g., RDF, OWL, SKOS).</p> </li> <li> <p>Metadata schemas    How resources are described in a structured way (e.g., Dublin Core, EDM, DCAT).</p> </li> <li> <p>Media formats    How images, 3D, audio, and video are stored and delivered.</p> </li> <li> <p>Exchange and packaging    How collections are bundled and transferred with integrity (e.g., BagIt, RO-Crate).</p> </li> <li> <p>Rights and licensing    How permissions, restrictions, and legal constraints are expressed in a machine-readable way.</p> </li> </ol>"},{"location":"standards-catalogue/data-architecture-standards-and-formats-3.1/#interoperability-levels-and-selection-of-standards","title":"Interoperability levels and selection of standards","text":"<p>The CH Cloud defines three interoperability levels:</p> <ul> <li>Level 1 (Basic) \u2013 minimal metadata for discovery; simple formats accepted</li> <li>Level 2 (Enhanced) \u2013 richer metadata with semantic alignment and structured formats</li> <li>Level 3 (Full) \u2013 complete semantic integration using graph-based Linked Data and shared ontologies</li> </ul> <p>Different standards are better suited to different levels. In the standards catalogue pages linked from this chapter we indicate where a standard is primarily relevant for Level 1, 2, or 3.</p>"},{"location":"standards-catalogue/data-architecture-standards-and-formats-3.1/#requirements-referenced-from-d62-informative-index","title":"Requirements referenced from D6.2 (informative index)","text":"<p>D6.2 \u00a73.1 contains normative requirement tables for: - REQ\u2013DS\u2013*: data service endpoint requirements (e.g., standards-based interfaces, avoidance of proprietary formats, access control, searchability, persistence) - REQ\u2013MD\u2013*: dataset metadata requirements (binding metadata to data via persistent identifiers; provenance; licensing; temporal/spatial validity; creation metadata) - REQ\u2013DQ\u2013*: data quality requirements (completeness, versioning, schema declaration/validation, global identifiers)</p> <p>This wiki page does not restate those tables. Instead: - treat the tables in the PDF as the authoritative source, and - use the pages below as implementation guidance to satisfy them consistently.</p> <p>Recommended follow-up pages in this catalogue: - Metadata standards (D6.2 \u00a73.1.2) - Structured &amp; semi-structured formats (D6.2 \u00a73.1.3) - Image &amp; raster formats (D6.2 \u00a73.1.4) - 3D &amp; spatial data (D6.2 \u00a73.1.5) - Audiovisual formats (D6.2 \u00a73.1.6) - Annotation, linking, aggregation (D6.2 \u00a73.1.7) - Packaging &amp; exchange formats (D6.2 \u00a73.1.8)</p>"},{"location":"standards-catalogue/data-architecture-standards-and-formats-3.1/#practical-guidance-how-to-use-this-section","title":"Practical guidance (how to use this section)","text":"<ol> <li>Start from your target interoperability level (L1/L2/L3).</li> <li>Identify the minimum metadata and interface obligations in REQ\u2013DS/REQ\u2013MD/REQ\u2013DQ.</li> <li>Select the most appropriate standards from the relevant pages (metadata, APIs, media formats).</li> <li>Document mappings and transformations (especially when onboarding from legacy formats).</li> <li>Prefer stable identifiers (URIs/PIDs) and machine-readable rights, and preserve them across workflows.</li> </ol>"},{"location":"standards-catalogue/deployment-and-infrastructure/","title":"Deployment and infrastructure (from D6.2 \u00a73.3)","text":"<p>Interoperability depends not only on data formats and APIs, but also on consistent deployment and infrastructure practices. Standardised, scalable, and modular deployment approaches enable services to integrate across heterogeneous platforms and provider environments.</p> <p>!!! note \"Normative boundary\" The official deliverable (PDF) defines the authoritative interoperability requirements. This page provides maintained implementation guidance and reference material; it does not introduce new MUST/SHALL obligations.</p>"},{"location":"standards-catalogue/deployment-and-infrastructure/#infrastructure-models-and-deployment-patterns","title":"Infrastructure models and deployment patterns","text":""},{"location":"standards-catalogue/deployment-and-infrastructure/#on-premise-deployment","title":"On-premise deployment","text":"<p>On-premise deployments are often required for high data sensitivity or strict institutional control. Interoperability is achieved by: - exposing services through stable, standards-based APIs, - adopting consistent identity and access mechanisms, - using repeatable deployment patterns that can be audited and reproduced.</p>"},{"location":"standards-catalogue/deployment-and-infrastructure/#cloud-native-deployment","title":"Cloud-native deployment","text":"<p>Public cloud providers (e.g., AWS, Azure, GCP) enable scalable, globally accessible services. Interoperability across cloud boundaries benefits from: - portable packaging and runtime assumptions (containers), - standard orchestration and service discovery patterns, - consistent contract and versioning practices across providers.</p>"},{"location":"standards-catalogue/deployment-and-infrastructure/#standards-and-frameworks-supporting-interoperable-deployment","title":"Standards and frameworks supporting interoperable deployment","text":""},{"location":"standards-catalogue/deployment-and-infrastructure/#open-container-and-orchestration-standards","title":"Open container and orchestration standards","text":"<ul> <li>OCI (Open Container Initiative): specifications for container images and runtimes</li> <li>Kubernetes: de facto standard for orchestration of containerised services</li> </ul> <p>Common ecosystem tooling (examples; not mandatory): - Helm: packaging and release management for Kubernetes - Istio (service mesh): traffic management, observability, policy enforcement (where justified) - KubeEdge: edge-native deployment patterns (where edge use cases exist)</p>"},{"location":"standards-catalogue/deployment-and-infrastructure/#infrastructure-as-code-iac-and-automation","title":"Infrastructure-as-Code (IaC) and automation","text":"<p>Declarative IaC tools support repeatable provisioning and reduce environment drift across providers: - Terraform, Ansible, Pulumi (representative examples)</p> <p>Recommended practice: - treat IaC as versioned artefacts in source control, - provide environment-specific overlays without diverging core logic, - document prerequisites and assumptions explicitly.</p>"},{"location":"standards-catalogue/deployment-and-infrastructure/#cicd-and-devops-pipelines","title":"CI/CD and DevOps pipelines","text":"<p>Version-controlled deployment workflows support repeatability and traceability: - GitOps approaches (e.g., Argo CD, Flux) for declarative, audited deployments - CI/CD tools (e.g., Jenkins, GitLab CI) to standardise build/test/deploy pipelines</p> <p>Recommended practice: - use consistent pipeline stages (build \u2192 test \u2192 security checks \u2192 deploy), - publish release artefacts and deployment manifests with version identifiers, - keep secrets out of repositories; integrate with a secrets manager.</p>"},{"location":"standards-catalogue/deployment-and-infrastructure/#platform-standards-enabling-interoperability-selected-references","title":"Platform standards enabling interoperability (selected references)","text":""},{"location":"standards-catalogue/deployment-and-infrastructure/#fiware-and-ngsi","title":"FIWARE and NGSI","text":"<p>FIWARE provides open-source components for context information management and is commonly deployed via Docker/Kubernetes. It includes the NGSI API family, used in some data space and smart systems contexts.</p>"},{"location":"standards-catalogue/deployment-and-infrastructure/#onem2m-and-etsi-standards","title":"oneM2M and ETSI standards","text":"<ul> <li>oneM2M: reference models for interoperability across IoT platforms</li> <li>ETSI NFV and ETSI MEC: architectures for virtualised network functions and edge computing</li> </ul> <p>!!! tip \"Scope guidance\" Treat FIWARE/NGSI, oneM2M, and ETSI references as optional, context-driven standards. If they become relevant for CH Cloud components (e.g., sensor/time-series or edge scenarios), document the rationale, integration boundary, and interoperability contracts.</p>"},{"location":"standards-catalogue/deployment-and-infrastructure/#practical-checklist-for-teams","title":"Practical checklist (for teams)","text":"<ul> <li>Use container images that are OCI-compliant and reproducible.</li> <li>Prefer Kubernetes-native deployment patterns for portability (when orchestration is required).</li> <li>Manage infrastructure with versioned IaC.</li> <li>Use CI/CD (or GitOps) to make deployments repeatable and auditable.</li> <li>Keep API contracts and versioning explicit and stable across environments.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/","title":"Protocols and APIs (from D6.2 \u00a73.2)","text":"<p>Protocols and APIs define how systems communicate, how data flows between services, and how external tools interact with CH Cloud components. They are essential for reproducible workflows, consistent metadata harvesting, cross-institutional access to digital assets, and integration with existing cultural heritage and research infrastructures.</p> <p>!!! note \"Normative boundary\" The official deliverable (PDF) defines the authoritative interoperability requirements. This page provides maintained implementation guidance and reference material; it does not introduce new MUST/SHALL obligations.</p>"},{"location":"standards-catalogue/protocols-and-apis/#what-protocol-choices-affect-in-the-ch-cloud","title":"What protocol choices affect in the CH Cloud","text":"<p>Protocol choices directly influence: - how datasets are ingested and synchronised, - how digital objects (images, 3D, audiovisual) are delivered, - how metadata is harvested and exposed to external platforms, - how user-facing applications interact with underlying services, - how semantic data is queried or published, - how distributed services remain discoverable and interoperable.</p>"},{"location":"standards-catalogue/protocols-and-apis/#quick-selection-guide-practical-when-to-use-what","title":"Quick selection guide (practical \u201cwhen to use what\u201d)","text":"<ul> <li>IIIF: interoperable delivery and presentation of image-based collections (manifests, deep zoom, annotations).</li> <li>OAI-PMH: legacy-friendly metadata harvesting and incremental synchronisation.</li> <li>REST: baseline for service interoperability and integration with most clients.</li> <li>GraphQL: flexible field selection for complex/nested domain objects via a single endpoint.</li> <li>SPARQL: RDF/knowledge graph querying and semantic federation (Levels 2\u20133).</li> <li>OGC services: interoperable delivery of geospatial layers and map services.</li> <li>Solid: user-controlled data pods and read/write Linked Data for personal workspaces and user-generated content.</li> <li>OpenAPI / JSON Schema / SHACL: contracts and validation artefacts enabling machine-testable interoperability.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#321-iiif-international-image-interoperability-framework","title":"3.2.1 IIIF \u2013 International Image Interoperability Framework","text":"<p>IIIF is a set of REST-based APIs enabling consistent access to high-resolution images and associated metadata across institutions. It defines predictable URLs and JSON-LD\u2013based manifests describing digital objects, sequences, annotations, and structure.</p>"},{"location":"standards-catalogue/protocols-and-apis/#recommended-use","title":"Recommended use","text":"<ul> <li>web-accessible images of cultural heritage objects (manuscripts, artworks, photos, maps),</li> <li>deep zoom, region selection, tiling, multi-resolution viewing,</li> <li>annotation workflows (crowdsourcing, scholarly annotation),</li> <li>integration with existing IIIF viewers (e.g., Mirador, Universal Viewer),</li> <li>synchronisation patterns using the Change Discovery API where applicable.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#when-not-to-use","title":"When not to use","text":"<ul> <li>non-image data (3D, audio, text) unless wrapped in IIIF Presentation manifests,</li> <li>preservation-grade delivery of original master files (e.g., TIFF masters),</li> <li>cases where only thumbnails/low-resolution previews are needed.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#ch-cloud-relevance","title":"CH Cloud relevance","text":"<ul> <li>strong candidate for Level 1\u20132 interoperability for image-based collections,</li> <li>enables interoperable viewer/tool integration and annotation components.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#technical-notes","title":"Technical notes","text":"<ul> <li>IIIF Image API: region/size/quality/rotation operations</li> <li>IIIF Presentation API: JSON-LD manifests describing structure and metadata</li> <li>integrates with Web Annotation</li> <li>stable URLs for images and manifests are a prerequisite.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#322-oai-pmh-open-archives-initiative-protocol-for-metadata-harvesting","title":"3.2.2 OAI-PMH \u2013 Open Archives Initiative Protocol for Metadata Harvesting","text":"<p>OAI-PMH is an HTTP-based protocol for structured, incremental harvesting of metadata, commonly used by libraries, archives, and aggregators.</p>"},{"location":"standards-catalogue/protocols-and-apis/#recommended-use_1","title":"Recommended use","text":"<ul> <li>harvesting metadata from legacy repositories and OAI-compatible aggregators,</li> <li>periodic synchronisation of provider metadata,</li> <li>Level 1 ingestion where minimal transformation is sufficient,</li> <li>providers unable to expose REST/JSON(-LD) APIs.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#when-not-to-use_1","title":"When not to use","text":"<ul> <li>real-time synchronisation or event-driven updates,</li> <li>complex semantic structures (e.g., CIDOC-CRM, SKOS) without additional transformation,</li> <li>cases where richer APIs and formats are readily available.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#ch-cloud-relevance_1","title":"CH Cloud relevance","text":"<ul> <li>provides a compatibility layer for institutions without modern APIs,</li> <li>supports automated ingestion pipelines for baseline onboarding.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#technical-notes_1","title":"Technical notes","text":"<ul> <li>common XML metadata formats: Dublin Core, MODS, MARCXML, EDM</li> <li>verbs: Identify, ListRecords, ListSets, GetRecord, etc.</li> <li>large repositories: resumption tokens for pagination</li> <li>datestamp consistency is essential for incremental harvesting.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#323-solid-social-linked-data-protocol","title":"3.2.3 Solid \u2013 Social Linked Data Protocol","text":"<p>Solid is a W3C initiative for decentralised, user-controlled data storage using Pods (personal online data stores). It uses Linked Data principles and standard web technologies for read/write access to structured data.</p>"},{"location":"standards-catalogue/protocols-and-apis/#recommended-use_2","title":"Recommended use","text":"<ul> <li>user-owned data scenarios (private annotations, personal preferences),</li> <li>research environments where individuals curate personal metadata/observations,</li> <li>components requiring read/write interaction with Linked Data resources.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#when-not-to-use_2","title":"When not to use","text":"<ul> <li>general-purpose metadata or media delivery,</li> <li>bulk ingestion of institutional datasets,</li> <li>cases where stable, institution-controlled APIs are required.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#ch-cloud-relevance_2","title":"CH Cloud relevance","text":"<ul> <li>promising for future user-generated content, personal workspaces, and privacy-centric services,</li> <li>not intended as a mandatory institutional delivery mechanism.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#technical-notes_2","title":"Technical notes","text":"<ul> <li>RDF-native storage patterns</li> <li>standard HTTP methods (GET/POST/PATCH)</li> <li>identity via WebID; authentication via Solid-OIDC</li> <li>notification mechanisms can support near real-time updates</li> <li>stable URIs remain a core prerequisite.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#324-rest-apis","title":"3.2.4 REST APIs","text":"<p>REST is the dominant architectural style for web APIs. It uses standard HTTP methods (GET/POST/PUT/DELETE) and commonly returns JSON or JSON-LD.</p>"},{"location":"standards-catalogue/protocols-and-apis/#recommended-use_3","title":"Recommended use","text":"<ul> <li>exposing dataset metadata, records, validation results, and service functionality,</li> <li>building front-end applications that rely on structured data,</li> <li>baseline API interoperability (Levels 1\u20132),</li> <li>partner integration where technical capacity varies.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#when-not-to-use_3","title":"When not to use","text":"<ul> <li>graph-native semantic querying (use SPARQL for RDF graphs),</li> <li>highly complex nested retrieval that causes over/under-fetching (consider GraphQL),</li> <li>very large binary transfers where streaming/tiling is needed (use specialised protocols).</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#technical-notes_3","title":"Technical notes","text":"<ul> <li>document contracts with OpenAPI</li> <li>validate payloads with JSON Schema</li> <li>explicit API versioning is strongly recommended.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#325-graphql","title":"3.2.5 GraphQL","text":"<p>GraphQL is a query language for APIs that lets clients request exactly the data they need, typically via a single endpoint. It can reduce over/under-fetching and support nested object graphs efficiently.</p>"},{"location":"standards-catalogue/protocols-and-apis/#recommended-use_4","title":"Recommended use","text":"<ul> <li>clients need precise control over fields and nested structures,</li> <li>portals/tools require rich hierarchical views,</li> <li>Level 2 APIs offering complex data structures.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#when-not-to-use_4","title":"When not to use","text":"<ul> <li>simple or flat metadata (REST is simpler),</li> <li>direct RDF/graph querying (SPARQL is appropriate),</li> <li>streaming or binary transfers.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#technical-notes_4","title":"Technical notes","text":"<ul> <li>requires a formal schema</li> <li>resolvers must be implemented per field</li> <li>versioning and schema evolution must be managed carefully.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#326-sparql-protocol","title":"3.2.6 SPARQL Protocol","text":"<p>SPARQL is the W3C standard query language and HTTP protocol for retrieving and manipulating RDF data. It is central to knowledge graphs and semantic federation.</p>"},{"location":"standards-catalogue/protocols-and-apis/#recommended-use_5","title":"Recommended use","text":"<ul> <li>querying RDF/knowledge graphs,</li> <li>semantic metadata retrieval and federated cross-institution queries,</li> <li>Level 3 semantic integration, contextual navigation, semantic search.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#when-not-to-use_5","title":"When not to use","text":"<ul> <li>non-RDF datasets,</li> <li>simple key-value or tabular use cases where simpler APIs suffice.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#technical-notes_5","title":"Technical notes","text":"<ul> <li>SPARQL 1.1 features: federation, aggregates, property paths</li> <li>result formats: JSON, XML, CSV/TSV</li> <li>requires stable URIs and consistent ontologies.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#327-ogc-web-services-wms-wfs-wmts","title":"3.2.7 OGC Web Services (WMS, WFS, WMTS)","text":"<p>OGC standards support interoperable delivery of maps (WMS), vector features (WFS), and map tiles (WMTS), enabling GIS tool compatibility and mapping interfaces.</p>"},{"location":"standards-catalogue/protocols-and-apis/#recommended-use_6","title":"Recommended use","text":"<ul> <li>exposing geospatial layers and archaeological/spatial datasets,</li> <li>map-based discovery and visualisation in CH Cloud tools,</li> <li>Level 1\u20132 spatial interoperability.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#when-not-to-use_6","title":"When not to use","text":"<ul> <li>non-spatial datasets,</li> <li>cases where simple vector-only JSON formats (e.g., GeoJSON) are sufficient and no GIS interoperability is required.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#ch-cloud-relevance_3","title":"CH Cloud relevance","text":"<ul> <li>important where CH datasets include spatial information (sites, historic maps, coordinates),</li> <li>compatible with widely used GIS ecosystems.</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#328-api-description-and-validation-standards","title":"3.2.8 API description and validation standards","text":"<p>Interoperability requires predictable service behaviour, machine-readable contracts, and consistent validation.</p>"},{"location":"standards-catalogue/protocols-and-apis/#when-to-use","title":"When to use","text":"<ul> <li>OpenAPI for documenting REST contracts</li> <li>JSON Schema for validating JSON payloads</li> <li>SHACL for validating RDF graphs and semantic constraints</li> </ul>"},{"location":"standards-catalogue/protocols-and-apis/#ch-cloud-relevance_4","title":"CH Cloud relevance","text":"<ul> <li>foundational for onboarding tools, metadata validators, and automated compliance checks,</li> <li>enables machine-testable interoperability across services.</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/","title":"Security and data privacy (from D6.2 \u00a73.4)","text":"<p>Security and data privacy are foundational to interoperability within the Cultural Heritage Cloud (CH Cloud). Distributed services, applications, data pipelines, and catalogues can only interoperate reliably when they share a common trust framework, including: - unified identity and authentication mechanisms, - consistent authorisation rules, - secure transport and storage practices, - machine-actionable rights and access restrictions, - reproducible deployment security practices, - verifiable audit and provenance metadata.</p> <p>!!! note \"Normative boundary\" The official deliverable (PDF) defines the authoritative requirements and compliance rules. This page provides maintained implementation guidance and reference material for consistent implementation across partners and environments. Legal and organisational aspects are handled in D6.1.</p>"},{"location":"standards-catalogue/security-and-data-privacy/#341-authentication-infrastructure-aai-egi-check-in-as-identity-provider","title":"3.4.1 Authentication infrastructure (AAI): EGI Check-in as Identity Provider","text":"<p>Baseline expectation: services requiring authentication integrate with EGI Check-in to ensure unified login experience and consistent identity semantics across the infrastructure.</p>"},{"location":"standards-catalogue/security-and-data-privacy/#capabilities-relevant-for-interoperability","title":"Capabilities relevant for interoperability","text":"<ul> <li>OIDC: standard authentication for web and API clients</li> <li>OAuth 2.0: token-based authorisation for services and automation</li> <li>SAML 2.0: integration with eduGAIN and institutional IdPs (where required)</li> <li>JWT access tokens: self-contained tokens enabling local verification</li> <li>Account linking: supports multiple IdPs under one stable identity</li> <li>Group/role attributes: enables downstream policy enforcement</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#required-identity-semantics","title":"Required identity semantics","text":"<ul> <li>each user is represented by a persistent unique subject identifier (<code>sub</code>)</li> <li>linked identities are merged under a single stable identity</li> </ul> <p>Implementation guidance: - avoid creating local user identities that conflict with Check-in\u2019s global subject identifiers - use the stable identity for access control, provenance metadata, audit trails, and membership history</p>"},{"location":"standards-catalogue/security-and-data-privacy/#supported-protocols","title":"Supported protocols","text":"<ul> <li>OpenID Connect (OIDC): recommended for web and API services</li> <li>OAuth2: service-to-service and delegated authorisation</li> <li>SAML2: backward compatibility where required</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#supported-authentication-flows","title":"Supported authentication flows","text":"<p>Services should support one or more flows as appropriate: - Authorization Code Flow: interactive web portals and browser clients - Client Credentials Flow: background jobs and microservices (service-to-service) - Device Authorization Flow: CLI tools and limited UI devices - Refresh Token Flow: session renewal for long-running apps</p>"},{"location":"standards-catalogue/security-and-data-privacy/#token-lifetimes-defaults","title":"Token lifetimes (defaults)","text":"<ul> <li>access token: 1 hour</li> <li>refresh token: up to 30 days   Token lifetimes are enforced by the IdP and should not be extended by clients.</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#required-token-validation-behaviour","title":"Required token validation behaviour","text":"<p>Every service should validate: - signature (via JWKS) - issuer (<code>iss</code>) - audience (<code>aud</code>) - expiry (<code>exp</code>) - not-before (<code>nbf</code>) - required scopes/entitlements</p> <p>Security rule: tokens must not be logged or stored in plaintext.</p>"},{"location":"standards-catalogue/security-and-data-privacy/#342-authorisation-models-rbacabac-and-policy-enforcement","title":"3.4.2 Authorisation models (RBAC/ABAC and policy enforcement)","text":"<p>Authorisation determines what authenticated users/systems may do. A consistent model is based on: - Check-in entitlements (roles), - group membership (virtual organisations and subgroups), - resource-level rights metadata, - optional attribute-based restrictions.</p>"},{"location":"standards-catalogue/security-and-data-privacy/#role-and-group-model","title":"Role and group model","text":"<p>Check-in provides role/group membership via entitlement strings.</p> <p>Typical entitlement patterns (examples): - <code>urn:mace:egi.eu:group:vo.chcloud.eu:role=member#aai.egi.eu</code> - <code>urn:mace:egi.eu:group:vo.chcloud.eu:role=admin#aai.egi.eu</code> - <code>urn:mace:egi.eu:group:vo.chcloud.eu:collection-x:role=curator#aai.egi.eu</code></p> <p>Entitlements encode: - VO identifier, - optional subgroup (domain/collection/project), - role, - authoritative namespace.</p>"},{"location":"standards-catalogue/security-and-data-privacy/#required-authorisation-behaviour","title":"Required authorisation behaviour","text":"<ul> <li>extract roles/groups from OIDC entitlements</li> <li>map entitlements to local permissions using documented rules</li> <li>avoid maintaining independent/conflicting role stores</li> <li>enforce consistent responses:<ul> <li><code>401 Unauthorized</code>: missing/invalid token</li> <li><code>403 Forbidden</code>: valid token but insufficient permissions</li> </ul> </li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#rbac-and-abac-support","title":"RBAC and ABAC support","text":"<ul> <li>RBAC: roles define broad capabilities (viewer/editor/curator/admin)</li> <li>ABAC: attributes constrain access (sensitivity, embargo, rights, affiliation)   Services may combine both (e.g., role grants write; attribute gates resource access).</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#machine-to-machine-authorisation","title":"Machine-to-machine authorisation","text":"<ul> <li>OAuth2 Client Credentials flow</li> <li>short-lived JWTs</li> <li>mTLS where appropriate for internal service links</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#343-api-security-standards","title":"3.4.3 API security standards","text":"<p>APIs are the primary interoperability mechanism and must behave securely across institutional boundaries.</p>"},{"location":"standards-catalogue/security-and-data-privacy/#transport-security","title":"Transport security","text":"<ul> <li>HTTPS/TLS 1.2+ for all services</li> <li>disable plain HTTP (redirect or reject)</li> <li>prefer strong cipher suites; avoid deprecated algorithms</li> <li>internal services may enforce mTLS where appropriate</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#token-handling","title":"Token handling","text":"<ul> <li>accept only OIDC JWTs issued by EGI Check-in (where auth applies)</li> <li>pass tokens via <code>Authorization: Bearer &lt;token&gt;</code></li> <li>cookies only for browser flows and must be <code>Secure</code> + <code>HttpOnly</code></li> <li>never log access tokens</li> <li>validate signature/time/audience/expiry</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#api-authorisation-patterns","title":"API authorisation patterns","text":"<ul> <li>Method-level rules: endpoint-by-endpoint role requirements</li> <li>Resource-level rules: rights/attributes per resource (public/restricted/embargoed/sensitive)</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#standard-security-headers-configurable","title":"Standard security headers (configurable)","text":"<ul> <li>Strict-Transport-Security</li> <li>Content-Security-Policy</li> <li>X-Content-Type-Options</li> <li>Referrer-Policy</li> <li>Cross-Origin-Resource-Policy</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#344-data-privacy-and-protection-gdpr-supporting-technical-practices","title":"3.4.4 Data privacy and protection (GDPR-supporting technical practices)","text":"<p>This documentation does not define legal obligations; it specifies technical mechanisms that support compliance and consistent interoperable behaviour.</p>"},{"location":"standards-catalogue/security-and-data-privacy/#data-minimisation-and-redaction","title":"Data minimisation and redaction","text":"<ul> <li>avoid exposing personal data unnecessarily</li> <li>provide configurable redaction pipelines</li> <li>explicitly mark sensitive fields where applicable</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#pseudonymisation","title":"Pseudonymisation","text":"<ul> <li>use persistent pseudonyms for internal linking</li> <li>avoid embedding personal identifiers in filenames, URLs, manifests</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#logging-and-auditability","title":"Logging and auditability","text":"<p>Logs should include: - timestamp - user/service identity (token claims) - operation performed - resource identifier</p> <p>Logs must not include: - access tokens - passwords - full payloads containing personal data (unless explicitly justified and protected)</p>"},{"location":"standards-catalogue/security-and-data-privacy/#provenance-for-privacy-decisions","title":"Provenance for privacy decisions","text":"<p>Maintain provenance trails for: - which service produced/modified data - how transformations impacted personal data - which access policies were applied</p>"},{"location":"standards-catalogue/security-and-data-privacy/#345-rights-management-and-technical-enforcement","title":"3.4.5 Rights management and technical enforcement","text":"<p>Rights and licence metadata may drive technical enforcement decisions (access control and workflow restrictions such as embargoes or culturally sensitive material).</p> <p>Implementation guidance: services performing authorisation/policy enforcement should be able to consume machine-readable rights/licence metadata and apply consistent enforcement when required.</p> <p>Example mapping: - Public domain / CC0: full download; derivatives permitted - CC BY: sharing allowed; attribution must propagate in metadata - Restricted: block full-resolution; allow previews - Embargoed: hide until embargo expiry - Culturally sensitive: restrict to specific groups</p> <p>(See the dedicated \u201crights and licensing\u201d guidance page for vocabularies and mapping patterns.)</p>"},{"location":"standards-catalogue/security-and-data-privacy/#346-security-in-deployment-and-infrastructure-links-to-33","title":"3.4.6 Security in deployment and infrastructure (links to \u00a73.3)","text":"<p>Deployment security impacts interoperability and trust.</p>"},{"location":"standards-catalogue/security-and-data-privacy/#container-and-runtime-security","title":"Container and runtime security","text":"<ul> <li>use signed OCI-compliant images</li> <li>perform vulnerability scanning in CI pipelines</li> <li>avoid hardcoded secrets in images or manifests</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#secret-management","title":"Secret management","text":"<ul> <li>use vault-based secret stores (e.g., Vault / KMS-backed secrets)</li> <li>rotate secrets regularly</li> <li>never embed secrets in repositories</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#network-security","title":"Network security","text":"<ul> <li>expose only required ports</li> <li>segment internal traffic</li> <li>protect external endpoints behind gateways/ingress controllers</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#monitoring-and-incident-response","title":"Monitoring and incident response","text":"<ul> <li>standardised logging formats (e.g., OpenTelemetry)</li> <li>health endpoints (e.g., <code>/healthz</code>, <code>/readyz</code>)</li> <li>support centralised alerting for downtime/anomalous activity</li> </ul>"},{"location":"standards-catalogue/security-and-data-privacy/#347-security-expectations-by-interoperability-level-overview","title":"3.4.7 Security expectations by interoperability level (overview)","text":"<ul> <li>Level 1 (Basic): HTTPS mandatory; optional authentication; basic access restrictions</li> <li>Level 2 (Enhanced): mandatory AAI integration (EGI Check-in); OIDC tokens; RBAC; protected APIs</li> <li>Level 3 (Full): fine-grained ABAC; rights-aware policy enforcement; provenance tracking; encrypted service-to-service communication (mTLS)</li> </ul>"},{"location":"standards-catalogue-old/","title":"Standards catalogue (from D6.2 Chapter 3)","text":"<p>This section is a maintained catalogue of standards, formats, and protocols referenced by the ECHOES / CH Cloud interoperability framework.</p> <p>!!! note \"Normative boundary\" The official deliverable (PDF) contains the authoritative interoperability requirements. This catalogue provides reference material and implementation-oriented guidance, and should not introduce new MUST/SHALL obligations.</p>"},{"location":"standards-catalogue-old/#how-to-use-this-catalogue","title":"How to use this catalogue","text":"<p>Use the pages in this section to: - identify relevant data standards and formats for onboarding and exchange, - select protocols and APIs that maximise interoperability across providers, - align deployments with widely adopted infrastructure conventions, - implement baseline security and privacy expectations consistently, - express rights and licensing as machine-readable metadata.</p>"},{"location":"standards-catalogue-old/#mapping-to-d62","title":"Mapping to D6.2","text":"<p>This catalogue is derived from D6.2 Chapter 3: - 3.1 Data standards and formats - 3.2 Protocols and APIs - 3.3 Deployment and infrastructure - 3.4 Security and data privacy - 3.5 Access and use/reuse in terms of legal interoperability</p>"},{"location":"standards-catalogue-old/api-protocols/","title":"Protocols and APIs (D6.2 \u00a73.2)","text":"<p>This page summarises protocols and API styles referenced for interoperable access, harvesting, querying, and exchange.</p>"},{"location":"standards-catalogue-old/api-protocols/#iiif-international-image-interoperability-framework","title":"IIIF (International Image Interoperability Framework)","text":"<p>IIIF provides standard APIs for image delivery and presentation, enabling consistent access patterns and viewer/tool interoperability.</p> <p>Recommended use - interoperable image access and presentation, - consistent delivery of derivatives and manifests across providers.</p>"},{"location":"standards-catalogue-old/api-protocols/#oai-pmh-open-archives-initiative-protocol-for-metadata-harvesting","title":"OAI-PMH (Open Archives Initiative Protocol for Metadata Harvesting)","text":"<p>OAI-PMH is an HTTP-based protocol for incremental harvesting of metadata (commonly XML records).</p>"},{"location":"standards-catalogue-old/api-protocols/#use-when","title":"Use when","text":"<ul> <li>harvesting metadata from legacy repositories and aggregators,</li> <li>periodic synchronisation is required,</li> <li>providers cannot expose modern REST/JSON APIs.</li> </ul>"},{"location":"standards-catalogue-old/api-protocols/#avoid-when","title":"Avoid when","text":"<ul> <li>real-time or event-driven synchronisation is required,</li> <li>complex semantic structures are required without additional transformation.</li> </ul>"},{"location":"standards-catalogue-old/api-protocols/#technical-notes","title":"Technical notes","text":"<ul> <li>Supports incremental updates via datestamps</li> <li>Uses resumption tokens for large sets (pagination)</li> <li>Common metadata formats include DC, MODS, MARCXML, EDM.</li> </ul>"},{"location":"standards-catalogue-old/api-protocols/#solid-social-linked-data-protocol","title":"Solid (Social Linked Data Protocol)","text":"<p>Solid is a W3C initiative for decentralised, user-controlled data storage using Pods, aligned with Linked Data principles.</p>"},{"location":"standards-catalogue-old/api-protocols/#use-when_1","title":"Use when","text":"<ul> <li>user-owned data is required (e.g., private annotations, preferences),</li> <li>read/write Linked Data interaction is needed.</li> </ul>"},{"location":"standards-catalogue-old/api-protocols/#avoid-when_1","title":"Avoid when","text":"<ul> <li>bulk institutional dataset delivery is needed,</li> <li>stable institution-controlled APIs are required.</li> </ul>"},{"location":"standards-catalogue-old/api-protocols/#rest-apis","title":"REST APIs","text":"<p>REST remains the default integration style for web services due to broad adoption and ecosystem tooling.</p> <p>Recommended use - interoperable service endpoints with stable resource identifiers, - predictable HTTP semantics for integration.</p>"},{"location":"standards-catalogue-old/api-protocols/#graphql","title":"GraphQL","text":"<p>GraphQL is useful where clients need flexible query shapes and aggregation within a single endpoint.</p> <p>Recommended use - client-driven query flexibility is a priority, - strong schema governance is available.</p>"},{"location":"standards-catalogue-old/api-protocols/#sparql-protocol","title":"SPARQL Protocol","text":"<p>SPARQL provides a standard query protocol for RDF graphs and knowledge graph endpoints.</p> <p>Recommended use - RDF-native querying and federation patterns, - semantic integration workflows.</p>"},{"location":"standards-catalogue-old/api-protocols/#ogc-web-services-wms-wfs-wmts","title":"OGC Web Services (WMS, WFS, WMTS)","text":"<p>OGC web services provide established interoperability patterns for geospatial layers and map services.</p> <p>Recommended use - GIS / spatial data interoperability and geospatial client compatibility.</p>"},{"location":"standards-catalogue-old/api-protocols/#api-description-and-validation-standards-d62-328","title":"API description and validation standards (D6.2 \u00a73.2.8)","text":"<p>Use standard, machine-readable API descriptions and validation artefacts: - OpenAPI for REST contract description - JSON Schema for payload structure (JSON) - SHACL for RDF constraint validation (semantic rules)</p>"},{"location":"standards-catalogue-old/data-standards-and-formats/","title":"Data standards and formats (D6.2 \u00a73.1)","text":"<p>This page provides a practical overview of data standards and formats commonly used in cultural heritage and relevant to CH Cloud interoperability.</p>"},{"location":"standards-catalogue-old/data-standards-and-formats/#1-semantic-web-and-graph-based-standards","title":"1) Semantic Web and graph-based standards","text":""},{"location":"standards-catalogue-old/data-standards-and-formats/#rdf-and-common-serialisations","title":"RDF and common serialisations","text":"<p>RDF is the core model for representing linked data and knowledge graphs. Common serialisations include: - Turtle / RDF/XML (RDF-native serialisations) - JSON-LD (web-friendly RDF serialisation)</p> <p>Recommended use - when publishing or integrating data into a knowledge graph, - when cross-institutional linking and semantic enrichment are required, - when SHACL-based validation is used.</p>"},{"location":"standards-catalogue-old/data-standards-and-formats/#json-ld","title":"JSON-LD","text":"<p>JSON-LD enables treating data as normal JSON while remaining compatible with RDF tooling (e.g., conversion to/from other RDF serialisations).</p> <p>Validation approach - JSON Schema for structural checks - SHACL for semantic constraints</p>"},{"location":"standards-catalogue-old/data-standards-and-formats/#2-structured-and-semi-structured-data-formats","title":"2) Structured and semi-structured data formats","text":""},{"location":"standards-catalogue-old/data-standards-and-formats/#csv-tsv","title":"CSV / TSV","text":"<p>Lightweight tabular formats for inventories, lists, registers, and bulk updates.</p> <p>Use when - providers export spreadsheet-style records, - Level 1 onboarding requires a minimal submission option, - simple transformation pipelines are used.</p> <p>Avoid when - hierarchical/nested structures must be preserved, - rich semantics and relationships are required (consider RDF/JSON-LD), - multilingual / ontology-aligned modelling is required.</p>"},{"location":"standards-catalogue-old/data-standards-and-formats/#xml-tei-eadeac-cpf-mets-mods-etc","title":"XML (TEI, EAD/EAC-CPF, METS, MODS, etc.)","text":"<p>XML supports rich hierarchical modelling and is widely used in libraries and archives.</p> <p>Use when - data is inherently hierarchical (e.g., manuscripts), - archival metadata requires EAD/EAC-CPF, - preservation-grade interchange is required.</p> <p>Avoid when - lightweight JSON APIs are required, - graph-based structures are needed without conversion.</p>"},{"location":"standards-catalogue-old/data-standards-and-formats/#3-media-and-domain-specific-formats-overview","title":"3) Media and domain-specific formats (overview)","text":""},{"location":"standards-catalogue-old/data-standards-and-formats/#images-and-raster-data","title":"Images and raster data","text":"<p>Prefer formats that support long-term preservation and broad tool support. When interoperability with image delivery services is required, align with IIIF-based access patterns (see Protocols &amp; APIs).</p>"},{"location":"standards-catalogue-old/data-standards-and-formats/#3d-and-spatial-data","title":"3D and spatial data","text":"<p>Use established 3D and GIS formats suited to your domain and workflows; prioritise formats that are widely supported, well-documented, and easy to validate.</p>"},{"location":"standards-catalogue-old/data-standards-and-formats/#audiovisual-formats","title":"Audiovisual formats","text":"<p>Prefer broadly supported formats for access and preservation workflows; document codec/container choices consistently.</p>"},{"location":"standards-catalogue-old/data-standards-and-formats/#4-annotation-linking-aggregation-and-packaging","title":"4) Annotation, linking, aggregation, and packaging","text":""},{"location":"standards-catalogue-old/data-standards-and-formats/#annotation-and-linking","title":"Annotation and linking","text":"<p>Where annotations are exchanged across systems, use web-friendly, URI-addressable resources and prefer standards that support linking to targets via resolvable identifiers.</p>"},{"location":"standards-catalogue-old/data-standards-and-formats/#packaging-and-exchange-formats","title":"Packaging and exchange formats","text":"<p>For reproducible workflows and exchange, prefer packaging approaches that can bundle payloads plus metadata and provenance (e.g., structured \u201ccrate\u201d approaches, where applicable).</p>"},{"location":"standards-catalogue-old/deployment-and-infrastructure/","title":"Deployment and infrastructure references (D6.2 \u00a73.3)","text":"<p>This page lists commonly adopted standards and tooling patterns for interoperable deployment across heterogeneous environments (on-premise and cloud).</p>"},{"location":"standards-catalogue-old/deployment-and-infrastructure/#deployment-models-high-level","title":"Deployment models (high-level)","text":"<ul> <li>On-premise deployment may be required for highly sensitive data where local control is mandatory.</li> <li>Cloud-native deployment supports scalable, globally accessible services.</li> </ul>"},{"location":"standards-catalogue-old/deployment-and-infrastructure/#open-container-and-orchestration-standards","title":"Open container and orchestration standards","text":"<ul> <li>OCI (Open Container Initiative) specifications for container formats and runtimes</li> <li>Kubernetes for orchestration of containerised services</li> <li>Common ecosystem tools: Helm (packaging), service mesh patterns (where applicable)</li> </ul>"},{"location":"standards-catalogue-old/deployment-and-infrastructure/#infrastructure-as-code-and-automation","title":"Infrastructure-as-Code and automation","text":"<p>Declarative infrastructure tooling supports repeatable deployments and consistent environments: - Terraform / Ansible / Pulumi (representative examples)</p> <p>!!! note Treat this page as a living reference list. Keep concrete runbooks, environment-specific examples, and operational scripts in the repository (not in the PDF deliverable).</p>"},{"location":"standards-catalogue-old/metadata-standards/","title":"Metadata standards (D6.2 \u00a73.1.2)","text":"<p>Metadata standards define how resources are described so they can be discovered, compared, and integrated across systems.</p> <p>In the CH Cloud context, metadata supports: - search and discovery, - dataset registration and cataloguing, - semantic integration and enrichment, - aggregation from external providers.</p>"},{"location":"standards-catalogue-old/metadata-standards/#dublin-core-dc-dcterms","title":"Dublin Core (DC / DCTERMS)","text":"<p>Dublin Core is a lightweight descriptive metadata standard and a common baseline across repositories.</p>"},{"location":"standards-catalogue-old/metadata-standards/#use-when","title":"Use when","text":"<ul> <li>only basic descriptive metadata is required (e.g., initial registration),</li> <li>Level 1 interoperability aims at discoverability,</li> <li>onboarding from legacy systems that cannot export richer schemas.</li> </ul>"},{"location":"standards-catalogue-old/metadata-standards/#avoid-when","title":"Avoid when","text":"<ul> <li>modelling complex CH semantics (events, actors, provenance) is required,</li> <li>object-level relationships must be expressed with domain depth.</li> </ul>"},{"location":"standards-catalogue-old/metadata-standards/#technical-notes","title":"Technical notes","text":"<ul> <li>Serialisations: XML (common in OAI-PMH), RDF (often with DCTERMS), JSON-LD (web APIs)</li> <li>Validation: XSD (XML), SHACL (RDF), JSON Schema (JSON/JSON-LD)</li> <li>Profiles can restrict mandatory/optional elements.</li> </ul>"},{"location":"standards-catalogue-old/metadata-standards/#dcat-data-catalog-vocabulary","title":"DCAT (Data Catalog Vocabulary)","text":"<p>DCAT is a W3C vocabulary for describing datasets, data services, and distributions in catalogues (it describes catalogue-level entities, not internal object-level semantics).</p>"},{"location":"standards-catalogue-old/metadata-standards/#use-when_1","title":"Use when","text":"<ul> <li>describing datasets/APIs/data services in a catalogue,</li> <li>enabling interoperable catalogue exchange and harvesting,</li> <li>Level 1\u20132 onboarding requires dataset/service registration.</li> </ul>"},{"location":"standards-catalogue-old/metadata-standards/#avoid-when_1","title":"Avoid when","text":"<ul> <li>detailed object/item description is required,</li> <li>domain ontologies (e.g., CIDOC-CRM) are needed for internal semantic depth.</li> </ul>"},{"location":"standards-catalogue-old/metadata-standards/#technical-notes_1","title":"Technical notes","text":"<ul> <li>Core classes: <code>dcat:Dataset</code>, <code>dcat:Distribution</code>, <code>dcat:DataService</code></li> <li>Serialisations: RDF/Turtle, JSON-LD</li> <li>Validation: SHACL shapes for application profiles; existing profiles such as DCAT-AP can be referenced.</li> </ul>"},{"location":"standards-catalogue-old/metadata-standards/#edm-europeana-data-model","title":"EDM (Europeana Data Model)","text":"<p>EDM is an RDF-based aggregation model used by Europeana to integrate metadata from libraries, archives, museums, and audiovisual collections.</p>"},{"location":"standards-catalogue-old/metadata-standards/#use-when_2","title":"Use when","text":"<ul> <li>interoperating with Europeana-like aggregation ecosystems,</li> <li>providers already have EDM exports and want to minimise conversion effort,</li> <li>establishing mapping paths to ontology-centric representations.</li> </ul>"},{"location":"standards-catalogue-old/metadata-standards/#avoid-when_2","title":"Avoid when","text":"<ul> <li>using EDM as the internal \u201ccore ontology\u201d if another semantic backbone is chosen.</li> </ul>"},{"location":"standards-catalogue-old/metadata-standards/#technical-notes_2","title":"Technical notes","text":"<ul> <li>Building blocks: OAI-ORE (aggregations/proxies), Dublin Core / DCTERMS, SKOS</li> <li>Stable URIs are essential for aggregation resources</li> <li>SHACL can be used for conformance checks pre/post transformation.</li> </ul>"},{"location":"standards-catalogue-old/rights-and-licensing/","title":"Rights and licensing metadata (D6.2 \u00a73.5)","text":"<p>This page provides reference guidance on expressing rights and reuse conditions as machine-readable metadata and translating them into predictable technical behaviours.</p>"},{"location":"standards-catalogue-old/rights-and-licensing/#purpose-of-legal-interoperability-in-the-ch-cloud","title":"Purpose of legal interoperability in the CH Cloud","text":"<p>In a distributed multi-provider environment, datasets originate from institutions operating under different rights, restrictions, and legal frameworks. Interoperability requires that: - legal constraints travel with data, - systems interpret them consistently, - access controls respect them automatically, - downstream services do not violate conditions unintentionally.</p>"},{"location":"standards-catalogue-old/rights-and-licensing/#machine-readable-rights-and-license-metadata","title":"Machine-readable rights and license metadata","text":"<p>Prefer widely adopted rights frameworks and URI-based expressions, including: - Creative Commons licenses (e.g., CC0, CC BY, CC BY-SA, CC BY-NC) - RightsStatements.org (cultural heritage focused statements) - DCTERMS rights metadata (e.g., <code>dcterms:rights</code>, <code>dcterms:license</code>) - IIIF <code>rights</code> property for image-based objects - Europeana/EDM rights properties (e.g., <code>edm:rights</code>)</p>"},{"location":"standards-catalogue-old/rights-and-licensing/#practical-guidance","title":"Practical guidance","text":"<ul> <li>ensure each digital object or dataset includes a clear rights statement</li> <li>express rights in machine-actionable form where possible (URI-based)</li> <li>support multiple rights where needed (e.g., one for metadata, one for media)</li> <li>preserve rights across transformation, ingestion, export, and aggregation</li> </ul>"},{"location":"standards-catalogue-old/rights-and-licensing/#legal-constraints-as-technical-access-rules-mapping-examples","title":"Legal constraints as technical access rules (mapping examples)","text":"Legal condition Typical technical behaviour Copyrighted content restricted download; watermarking; login required Non-commercial licence disable commercial-use workflows; usage disclaimers No-derivatives licence disable derivative generation (e.g., 3D reconstruction, AI enhancement) Embargoed material block access until embargo expiry; hide fields if required Sensitive cultural heritage role-based or affiliation-based restrictions No known copyright allow open access to high-resolution derivatives Donor/agreement-based restrictions project- or group-based access rules"},{"location":"standards-catalogue-old/rights-and-licensing/#personal-data-handling-gdpr-supporting-technical-obligations","title":"Personal data handling (GDPR-supporting technical obligations)","text":"<p>Where datasets contain personal data, systems should support: - pseudonymisation (stable identifiers; avoid personal data in URLs/identifiers) - data minimisation (process/store only what is necessary) - purpose limitation tagging where relevant - auditability without exposing personal data</p>"},{"location":"standards-catalogue-old/security-standards/","title":"Security and data privacy (D6.2 \u00a73.4)","text":"<p>This page summarises reference security standards and interoperability-oriented security practices required for consistent cross-provider trust.</p>"},{"location":"standards-catalogue-old/security-standards/#aai-egi-check-in-integration-d62-341","title":"AAI: EGI Check-in integration (D6.2 \u00a73.4.1)","text":"<p>All services requiring authentication should integrate with EGI Check-in to ensure consistent identity semantics across the infrastructure.</p>"},{"location":"standards-catalogue-old/security-standards/#capabilities-relevant-for-interoperability","title":"Capabilities relevant for interoperability","text":"<ul> <li>OIDC for web and API-based clients</li> <li>OAuth 2.0 for token-based authorisation and automation</li> <li>SAML 2.0 for institutional IdPs where required</li> <li>JWT access tokens for local verification</li> <li>account linking and group/role attributes for policy enforcement</li> </ul>"},{"location":"standards-catalogue-old/security-standards/#supported-authentication-flows","title":"Supported authentication flows","text":"<p>Services should support one or more of: - Authorization Code Flow (interactive browser login) - Client Credentials Flow (service-to-service) - Device Authorization Flow (CLI and limited UI devices) - Refresh Token Flow (session renewal)</p>"},{"location":"standards-catalogue-old/security-standards/#token-lifetimes-defaults","title":"Token lifetimes (defaults)","text":"<ul> <li>Access token: 1 hour</li> <li>Refresh token: up to 30 days</li> </ul>"},{"location":"standards-catalogue-old/security-standards/#required-token-validation-behaviour","title":"Required token validation behaviour","text":"<p>Services should validate: - signature (JWKS) - issuer (<code>iss</code>), audience (<code>aud</code>) - expiry (<code>exp</code>) and not-before (<code>nbf</code>) - required scopes/entitlements</p> <p>Tokens must not be logged or stored in plaintext.</p>"},{"location":"standards-catalogue-old/security-standards/#authorisation-models-d62-342","title":"Authorisation models (D6.2 \u00a73.4.2)","text":"<p>CH Cloud services typically combine: - RBAC (role-based access control) - ABAC (attribute-based access control) for sensitivity/embargo/rights/affiliation-based constraints</p> <p>Interoperable behaviour should clearly distinguish: - <code>401 Unauthorized</code> for missing/invalid token - <code>403 Forbidden</code> for valid token but insufficient permissions</p>"},{"location":"standards-catalogue-old/security-standards/#api-security-standards-d62-343","title":"API security standards (D6.2 \u00a73.4.3)","text":""},{"location":"standards-catalogue-old/security-standards/#transport-security","title":"Transport security","text":"<ul> <li>HTTPS / TLS (minimum TLS 1.2) for all services</li> <li>disable plain HTTP (redirect or reject)</li> <li>consider mTLS for service-to-service links where appropriate</li> </ul>"},{"location":"standards-catalogue-old/security-standards/#token-handling","title":"Token handling","text":"<ul> <li>accept OIDC (JWT) tokens issued by the federated IdP</li> <li>accept tokens via <code>Authorization: Bearer &lt;token&gt;</code> (cookies only for browser flows, secured)</li> <li>never store tokens in logs</li> <li>validate token signature/time/audience/expiry</li> </ul>"},{"location":"standards-catalogue-old/security-standards/#standard-security-headers","title":"Standard security headers","text":"<p>Implement configurable security headers (not hardcoded), e.g.: - Strict-Transport-Security - Content-Security-Policy - X-Content-Type-Options - Referrer-Policy - Cross-Origin-Resource-Policy</p>"},{"location":"standards-catalogue-old/security-standards/#data-privacy-technical-practices-gdpr-supporting-d62-344","title":"Data privacy technical practices (GDPR-supporting) (D6.2 \u00a73.4.4)","text":"<p>Interoperability requires privacy-related metadata and behaviours to be consistent across systems.</p> <p>Recommended technical practices: - data minimisation and redaction pipelines - pseudonymisation (use stable pseudonyms; avoid personal identifiers in URLs/manifests) - logging/auditability without exposing personal data</p>"}]}